We would like to extend our thanks to all reviewers for taking the time to engage with our work and provide feedback.

We made a few additions to the paper since the initial submission that we'd like to share with you. We've uploaded the updated pdf. In particular, we would like to draw your attention to the following:
- We've included additional baselines to the discriminative relational tasks experiments (section 4.1), including PrediNet and MLP baselines.
- We've extended the Abstractor framework by proposing several "symbol assignment" mechanisms, with different properties. This is described in section 2.3 "Symbol Assignment Mechanisms." As you know, the first step in the Abstractor model is to "assign a symbol to each object." In the previous version of the paper, we considered (what we are now calling) "positional symbols," where symbols are assigned to objects sequentially based on the order they appear. While simple, this method has the advantage of strictly obeying the relational bottleneck and it works well in our experiments. In section 2.3, we propose two other symbol assignment mechanisms: position relative symbols (which we mentioned in the previous version of the paper as well), and "symbolic attention". Symbolic attention enables symbols to cpme to represent information about the object's 'syntactic role', which can be shared across problem instances. That is, the symbol identifies an object in relational cross-attention based on its role, rather than only its position.
- In the math experiments of section 4.3, we've added an evaluation of an Abstractor model with symbolic attention. In the previous version, we compared an Abstractor with positional symbols to a standard Transformer, and observed consistent improvements. The Abstractor with symbolic attention yields a further improvement.

We hope you will look at these additions and provide any comments or questions you have. In addition, we have responded to each reviewer's original comments.  We look forward to hearing your thoughts.