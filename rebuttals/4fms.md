> The overly constrained setting limits the significance of the work. The authors conduct experiments under two settings: "purely relational" and "partially relational". As per the authors' definition, "purely relational" implies that object-level features are extraneous and that the statistics of relation/order are already sufficient for solving the task. This is an extremely restricted setting and may not fully represent the complexity of real-world tasks where both relational and object-level information are often important.

The importance of purely relational reasoning is suggested by the use of Raven's matrices in standard tests of human intelligence. Yet, we agree that "purely relational" tasks are not representative of the complexity of real-world tasks,
for which both relational and object-level information are often important. The motivation of the experiments on purely relational tasks is two-fold: 1) validate that the Abstractor does indeed achieve improved relational representation in a controlled setting with fewer confounding factors (e.g., relevance of object-level features), and 2) compare the Abstractor to existing relational architectures, which have so far focused on discriminative tasks in which there exists a latent set of relations that form a sufficient statistic (i.e., purely relational). Our main claim about the Abstractor is that it is able to learn good representations of relations. To test this claim, and isolate for the effect of the quality of relational representations, we believe more controlled purely relational tasks are appropriate. The experiments of section 4.1 and 4.2 attempt to do this for discriminative tasks and sequence-to-sequence tasks, respectively.

> The authors use math problem-solving to represent the "partially relational" setting, but the math problem here is arguably more relational/symbolic than object-level. It seems like the math problems here can be solved by symbolic rules.

The math problems are character-level sequence-to-sequence tasks, which require modeling both object-level features and relational features. The Transformer and Abstractor-based models are both neural architectures, which must learn to process the input as vector representations, integrating object-level and relational features. In our experiments, we observe that the incorporation of an Abstractor module into a neural architecture for this task yields meaningful performance benefits.

The math problems can of course be solved by symbolic rules, as you mention. But the Transformer and Abstractor are not symbolic programs. Their performance on such a task is a measure of their representational capacity and sample efficiency for modeling this kind of reasoning. Moreover, it is a longstanding problem to understand how the human brain carries out apparently symbolic 
computation such as the mathematical processing tasks in these experiments.

> Scalability is one of the most significant advantages of Transformer architectures. The performance can increase with the model size and data size. Given that the Abstractor is a variant of the Transformer, it's essential to determine whether the scaling law still applies to the Abstractor. From the results provided, the performance can outperform the vanilla Transformer when the data size increases from the 1000 - 5000 range. But what about using more data and a larger model size? Will the Abstractor consistently improve and outperform the Transformer?

In terms of computational scalability, it is relevant to point out that the Abstractor has the same advantages and disadvantages as a Transformer, due to the similarity in implementation. In particular, future advances in parallel computation and optimization of attention in Transformers can be directly applied to the Abstractor. We agree that testing scaling laws on Abstractor-based models would be an interesting future direction of research. 

In the object-sorting experiments of section 4.2, the task is saturated by the Abstractor at about 500 samples, and eventually saturated by the Transformer at about 3000 samples. Testing scaling of the Abstractor will require more complex tasks (e.g., language modeling) and considerable compute resources, which are out of scope for this project.

> Why did the authors choose to replace value vectors with input-independent vectors, while keeping the query and key vectors the same (Q -- X, K -- X, V -- S)? Would not the configuration (Q -- S, K -- S, V -- X) also disentangle object-level features (x) and symbolic vectors (s)? To me, the latter one is more intuitive: the relation weight R_{ij} between i, j is represented by the inner product of symbolic vectors, and then object-level features are weighted by R_{ij}.

Thanks for the question. We hope the following will help to clarify.

Consider first the case of symmetric relations. Given a pair of objects $x, y$, we think of $\langle \phi(x), \phi(y) \rangle$ as a relation between $x$ and $y$ in the following sense: 1) $\phi$ extracts features of the objects, and 2) the inner product compares the two features. The inner product will be large when the two feature vectors are similar, and small otherwise. In the case of asymmetric features, the feature extractor for each object can be different. With this in mind, relational cross-attention can be thought of as a form of message-passing where the message from object $j$ to object $i$ is $m_{j \to i} = (s_j, r(x_i, x_j))$, encoding the relation between the two objects and a symbol identifying the sender. In relational cross-attention, the symbols act as identifiers of the objects, but do not encode the objects' features.

In the configuration $(Q \gets S, K \gets S, V \gets X)$, the inner products would be $\langle s_i, s_j \rangle$. Since the set of symbols $S$ only represent the identity of the objects and not their features, the inner products between them would not represent relations between the input objects (in fact, the relations would be constant in the case of positional symbols). Moreover, since the values are the input objects ($V \gets X$), the object-level features are propagated through, and the relational bottleneck is not obeyed.

---------------------

Many thanks for your thoughtful engagement with our work! We hope this has addressed some of your questions and concerns. Please let us know if you have any other questions.