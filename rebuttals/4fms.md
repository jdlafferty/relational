> The overly constrained setting limits the significance of the work. The authors conduct experiments under two settings: "purely relational" and "partially relational". As per the authors' definition, "purely relational" implies that object-level features are extraneous and that the statistics of relation/order are already sufficient for solving the task. This is an extremely restricted setting and may not fully represent the complexity of real-world tasks where both relational and object-level information are often important.

We entirely agree that "purely relational" tasks are not representative of the complexity of real-world tasks. Indeed, in most real world tasks, both relational and object-level information are important. The motivation of the experiments on purely relational tasks is two fold: 1) validate that the Abstractor does indeed achieve improved relational representation in a controlled setting with fewer confounding factors (e.g., relevance of object-level features), and 2) compare the Abstractor to existing relational architectures, which have so far focused on discriminative tasks in which there exists a latent set of relations which form a sufficient statistic (i.e., purely relational). Our main claim about the Abstractor is that it is able to learn good representations of relations. To test this claim, and isolate for the effect of the quality of relational representations, we believe more controlled purely relational tasks are appropriate. The experiments of section 4.1 and 4.2 attempt to do this for discriminative tasks and sequence-to-sequence tasks, respectively.

> The authors use math problem-solving to represent the "partially relational" setting, but the math problem here is arguably more relational/symbolic than object-level. It seems like the math problems here can be solved by symbolic rules.

The math problems can of course be solved by symbolic rules. However, the task is modeled as a character-encoded sequence-to-sequence tasks. The inputs and outputs are sequences of characters, they are not symbolic. In order for a differentiable neural model to perform such a task, it does need to learn to integrate relational as well as object-level features. In this case, the "objects" are the characters.

> Scalability is one of the most significant advantages of Transformer architectures. The performance can increase with the model size and data size. Given that the Abstractor is a variant of the Transformer, it's essential to determine whether the scaling law still applies to the Abstractor. From the results provided, the performance can outperform the vanilla Transformer when the data size increases from the 1000 - 5000 range. But what about using more data and a larger model size? Will the Abstractor consistently improve and outperform the Transformer?

We agree that testing scaling laws on Abstractor-based models would be an interesting future direction of research. In terms of computational scalability, it is relevant to point out that the Abstractor has the same advantages and disadvantages as a Transformer, due to the similarity in implementation. In particular, for example, same parallelism in computation is possible and optimizations in computing attention can be directly applied to the Abstractor.

In the object-sorting experiments of section 4.2, the task is saturated by the Abstractor at about 500 samples, and eventually saturated by the Transformer at about 3000 samples. Testing scaling laws on the Abstractor (same as any other model) would require more complex tasks (e.g., language modeling) and considerable compute resources, which are out of scope for this project.

> Why did the authors choose to replace value vectors with input-independent vectors, while keeping the query and key vectors the same (Q -- X, K -- X, V -- S)? Would not the configuration (Q -- S, K -- S, V -- X) also disentangle object-level features (x) and symbolic vectors (s)? To me, the latter one is more intuitive: the relation weight R_{ij} between i, j is represented by the inner product of symbolic vectors, and then object-level features are weighted by R_{ij}.

Thanks for the question. We hope this will help clarify.

Consider first the case of symmetric relations. Given a pair of object $x, y$, we think of $\langle \phi(x), \phi(y) \rangle$ as a relation between $x$ and $y$ in the following sense: 1) $\phi$ extracts features of the objects, 2) the inner product compares the two features. The inner product will be large when the two features are similar, and small otherwise. In the case of asymmetric features, the feature extractor for each object can be different. With this in mind, relational cross-attention can be thought of as a form of message-passing where the message from object $j$ to object $i$ is $m_{j \to i} = (s_j, r(x_i, x_j))$ encodes the relation between the two objects and a symbol identifying the sender. In relational cross-attention, the symbols act as identifiers of the objects, but do not encode the objects' features.

In the configuration $(Q \gets S, K \gets S, V \gets X)$, the inner products would be $\langle s_i, s_j \rangle$. Since the set of symbols $S$ are input-independent, the inner products between them would not represent relations between the input objects (in fact, the relations would be constant). Moreover, since the values are the input objects ($V \gets X$), the object-level features are propagated through, and the relational bottleneck is not obeyed.

---------------------

Many thanks for your engagement with our work! We hope this has addressed some of your questions and concerns. Please let us know if you have any other questions.