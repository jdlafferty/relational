
%\newcommand{\MLP}{\text{MLP}}
%\newcommand{\FeedForward}{\text{FeedForward}}
%\newcommand{\Softmax}{\text{Softmax}}
%\newcommand{\reals}{\mathbb{R}}
\def\m{m}


\section{Symbolic message-passing algorithm}
\label{sec:message_passing_supplement}

As described in \Cref{sec:abstractor_framework}, 
we perform message-passing on these learned symbolic parameters according to the relation tensor $R$. In general, this message-passing operation can be described as a set-valued function of the form
\begin{equation}
    \label{eq:symbolic_message_passing_supp}
    s_i \leftarrow \text{Update}\Big( s_i, \ \left\{ \left(R[i,j], s_j\right)\right\}_{j\in[m]}\Big), \quad i = 1, \ldots, m
\end{equation}
That is, the value of the $i$th symbol is updated as a function of the set of tuples $(R[i,j], s_j)$ of the relations with all other objects and the symbols of these objects. The symbols $s_j$ are naturally viewed
as values on the nodes of a graph, and the relations $R[i,j]$ are naturally viewed as weights on the edges. A simple but important special case of this is
\begin{equation}
    \label{eq:linear_symbolic_mp:supp}
    s_i \leftarrow \sum_{j=1}^{m} R[i,j] s_j, \quad i=1, \ldots, m
\end{equation}
In the above, suppose that $d_r = 1$. Otherwise, the above operation is done for each dimension of the relation $R$ and the result is concatenated, as in multi-head attention.

Following message-passing, each updated symbol $s_i$, can be passed through a feedforward neural network $f:\reals^{d_s}\rightarrow \reals^{d_s}$ to compute a non-linear function of the output. %Empirically, a residual connection and layer normalization may be useful, as in a transformer.
This message-passing operation can be repeated multiple times to iteratively update the symbolic vectors.  The output of relational symbolic message-passing is the set of symbols $A$ at the end of this sequence of message-passing operations. The overall procedure is summarized in Algorithm~\ref{alg:relational_abstractor}. In Section~\ref{sec:function_spaces} we characterize the class of functions on relations that this operation can compute.

% JDC: Shouldn't $d_s$ be included in the list of hyperparameters below?
\begin{algorithm}[th!]
    \caption{Relational Abstractor}\label{alg:relational_abstractor}
    \SetKwFor{For}{for}{do}{end}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \SetKwInOut{LearnableParams}{Learnable parameters{\ }}
    \SetKwInOut{HyperParams}{Hyperparameters}

    \Input{Encoder entities: $E = (o_1, \ldots, o_\m) \in \mathbb{R}^{d_e \times m}$}
    \HyperParams{$L$ (number of layers), $H$ (number of heads), feedforward network structure}
    \LearnableParams{Symbols $S \in \reals^{d_s \times m}$, parameters $\{\theta_l\}$ of attention heads and feedforward models}
    \Output{Abstracted sequence: $A = (a_1, \ldots, a_\m) \in \reals^{d_a \times m}$}
    \vspace{1em}

    $A \gets S$

    \For{$l \gets 1$ \KwTo $L$}{
        $A \gets \text{SelfAttention}_{\theta_l}(A)$\;
        $A \gets \crossattend{E}{E}{A}{\theta_l}$\;
        $A \gets\FeedForward_{\theta_l}(A)$\;
        }
\end{algorithm}

\subsection{Computing the relation tensor with relational cross-attention}

Symbolic message passing is the first of the two main
%ingredients
components
of abstractors. What remains is to describe how the abstractor computes the relation tensor $R$. This is
done through a variant of transformer cross-attention that we refer to as \textit{relational cross-attention}.
To motivate this, we first describe how we can compute relations between pairs of objects through inner products.
The inner product operation is a natural way to capture notions of relations and similarity. In Euclidean space,
inner products capture the geometric alignment between vectors. Similarly, for objects with vector representations,
inner products between these vector representations can capture relations between these objects.

In general, we can formulate inner product relations as the standard Euclidean inner product between a pair of transformed object vectors. That is, 
\begin{equation}
    R(o_i, o_j) = \langle \phi(o_i), \psi(o_j) \rangle
    \label{eq:relation_innerproduct}
\end{equation}
This captures a large class of relation functions. In particular, the theory of reproducing kernel Hilbert spaces implies that any continuous, symmetric function on pairs of objects can be approximated with such functions
\citep{universal}. Multi-dimensional relation functions can be achieved by stacking multiple such inner products.
In the next section we frame this explicitly in terms of transformer operations.

