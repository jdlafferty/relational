\begin{abstract}
    Reasoning in terms of relations, analogies, and abstraction is a hallmark of human intelligence. An active debate is whether this relies on the use of symbolic processing or can be achieved using the same forms of function approximation that have been used for tasks such as image, audio, and, most recently, language processing.  We propose an intermediate
    % JDC: I THINK I HAD SUGGESTED WE INCLUDE "intermediate" HERE.  IF IT WAS DELETED FOR A REASON, I'M FINE WITH
    % THAT. BUT OTHERWISE, I THINK IT DOES COMMUNICTE THE IDEA THAT WE ARE TRYING TO "BRIDGET THE GAP."
    approach, motivated by principles of cognitive neuroscience, in which abstract symbols can emerge from
    distributed, neural representations under the influence of an inductive bias for learning that we refer to as a ``relational bottleneck.''  We present a framework that casts this inductive bias in terms of an extension of Transformers, in which specific types of attention mechanisms enforce the relational bottleneck and transform distributed symbols to implement a form of relational reasoning and abstraction. We theoretically analyze the class of relation functions the models can compute and empirically demonstrate superior sample-efficiency on relational tasks compared to standard Transformer architectures.
 \end{abstract}