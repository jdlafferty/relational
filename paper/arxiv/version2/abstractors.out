\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{The Abstractor Framework}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Relational symbolic message-passing}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Multi-head relations and relational cross-attention}{section.2}% 4
\BOOKMARK [2][-]{subsection.2.3}{Relational learning using transformers}{section.2}% 5
\BOOKMARK [2][-]{subsection.2.4}{Configuring abstractors for different tasks}{section.2}% 6
\BOOKMARK [1][-]{section.3}{Function classes}{}% 7
\BOOKMARK [2][-]{subsection.3.1}{Function class of inner product relations}{section.3}% 8
\BOOKMARK [2][-]{subsection.3.2}{Class of relational functions computable by symbolic message-passing}{section.3}% 9
\BOOKMARK [2][-]{subsection.3.3}{Composing abstractors to compute relations on relations}{section.3}% 10
\BOOKMARK [2][-]{subsection.3.4}{Robustness and error correction}{section.3}% 11
\BOOKMARK [2][-]{subsection.3.5}{Sparse, high-dimensional relations}{section.3}% 12
\BOOKMARK [1][-]{section.4}{Experiments}{}% 13
\BOOKMARK [2][-]{subsection.4.1}{Warm up: Ability to learn asymmetric and multi-dimensional relations}{section.4}% 14
\BOOKMARK [2][-]{subsection.4.2}{Superior sample-efficiency on relational tasks compared to plain transformers}{section.4}% 15
\BOOKMARK [2][-]{subsection.4.3}{Ability to generalize to similar tasks}{section.4}% 16
\BOOKMARK [2][-]{subsection.4.4}{Robustness and Out-of-Distribution generalization}{section.4}% 17
\BOOKMARK [2][-]{subsection.4.5}{Modularity and comparison to purely symbolic representations}{section.4}% 18
\BOOKMARK [1][-]{section.5}{Discussion}{}% 19
\BOOKMARK [1][-]{section*.6}{Acknowledgments}{}% 20
\BOOKMARK [1][-]{section*.8}{References}{}% 21
\BOOKMARK [1][-]{appendix.A}{Symbolic message-passing algorithm}{}% 22
\BOOKMARK [2][-]{subsection.A.1}{Computing the relation tensor with relational cross-attention}{appendix.A}% 23
