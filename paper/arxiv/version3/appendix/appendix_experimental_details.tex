\section{Experimental details}\label{sec:experimental_details}

In this section, we give further experimental details including architectures, hyperparameters, and implementation details. All model and experiments are implemented in Tensorflow. The code is publicly available on the project repo along with detailed experimental logs and instructions for reproduction. We implement all models ``from scratch'' in Tensorflow, using only built-in implementations of basic layers.

\subsection{Discriminative tasks (\Cref{ssec:experiments_discriminative})}

\subsubsection{Pairwise Order}
\textbf{Abstractor architecture} number of layers $L = 1$, relation dimension $d_r = 4$, symbol dimension $d_s = 64$, projection dimension $d_p = 16$, feedforward hidden dimension $d_{\mathrm{ff}} = 64$, relation activation function $\sigma_{\mathrm{rel}} = \mathrm{sigmoid}$. No layer normalization or residual connection. The symbols are learned parameters.

\textbf{CoRelNet Architecture} Given a sequence of objects, $X = (x_1, \ldots, x_\m)$, standard CoRelNet simply computes the inner product and takes the Softmax. We also add a learnable linear map, $W \in \reals^{d \times d}$. Hence, $R = \text{Softmax}(A), A = {\left[\langle W x_i, W x_j\rangle\right]}_{ij}$. The asymmetric variant of CoRelNet is given by $R = \text{Softmax}(A), A = {\left[\langle W_1 x_i, W_2 x_j\rangle\right]}_{ij}$, where $W_1, W_2 \in \reals^{d \times d}$ are learnable matrices.

\textbf{Training/Evaluation} We use the sparse categorical crossentropy loss and the Adam optimizer with a learning rate of $10^{-2}$, $\beta_1 = 0.9, \beta_2 = 0.999, \varepsilon = 10^{-7}$. We use a batch size of 64. We train for 100 epochs and restore the best model according to validation loss. We evaluate on the test set.

\subsubsection{SET}

The card images are RGB images of dimension $70 \times 50$. The CNN embedder produces embeddings fo dimension $d=64$ for each card. The CNN is trained to predict the four attributes of each card and then an embedding for each card is obtained from an intermediate layer (i.e., the parameters of the CNN are then frozen). Recall that the common architecture is $\texttt{CNN Embedder} \to \{\texttt{Abstractor} \text{ or } \texttt{CoRelNet}\} \to \texttt{Flatten} \to \texttt{Dense(2)}$. We tested against the standard version of CorelNet, but found that it did not learn anything. We iterated over the hyperparameters and architecture to improve its performance. We found that removing the softmax activation in CoRelNet improved performance a bit. We describe hyperparameters below.

\textbf{Common embedder's architecture} The architecture is given by \texttt{Conv2D} $\to$ \texttt{MaxPool2D} $\to$ \texttt{Conv2D} $\to$ \texttt{MaxPool2D} $\to$ \texttt{Flatten} $\to$ \texttt{Dense(64, 'relu')} $\to$ \texttt{Dense(64, 'relu')} $\to$ \texttt{Dense(2)}. The embedding is extracted from the penultimate layer. The CNN is trained to predict the four attributes of each card until it reaches perfect accuracy and near-zero loss.

\textbf{Abstractor architecture}
The Abstractor module has hyperparameters: number of layers $L = 1$, relation dimension $d_r = 4$, symmetric relations (i.e., $W_1^{(i)} = W_2^{(i)}$, $i \in [d_r]$), linear relation activation (i.e., $\sigma_{\mathrm{rel}}: x \mapsto x$), symbol dimension $d_s = 64$, projection dimension $d_p = 16$, feedforward hidden dimension $d_{\mathrm{ff}} = 128$, and no layer normalization or residual connection. The symbols are learned parameters.

\textbf{CoRelNet Architecture} Standard CoRelNet is described above. It simply computes, $R = \text{Softmax}(A), A = {\left[\langle W x_i, W x_j\rangle\right]}_{ij}$. This variant was stuck at 50\% accuracy regardless of training set size. We found that removing the Softmax helped.~\Cref{fig:exp_set_classification} compares against both variants of CoRelNet.

This finding suggests that allowing $\sigma_{\mathrm{rel}}$ to be a configurable hyperparameter is a useful feature of the Abstractor. Softmax performs contextual normalization of relations, such that the relation between $i$ and $j$ is normalized in terms of $i$'s relations with all other objects. This may be useful at times, but may also cripple a relational model when it is more useful to represent an absolute relation between a pair of objects, independently of the relations with other objects.

\textbf{Data generation} The data is generated by randomly sampling a SET with probability 1/2 and a non-SET with probability 1/2. The triplet of cards is then randomly shuffled.

\textbf{Training/Evaluation} We use the sparse categorical crossentropy loss and the Adam optimizer with a learning rate of $10^{-3}$, $\beta_1 = 0.9, \beta_2 = 0.999, \varepsilon = 10^{-7}$. We use a batch size of 64. We train for 200 epochs and restore the best model according to validation loss. We evaluate on the test set.


\subsection{Relational sequence-to-sequence tasks (\Cref{ssec:experiments_object_sorting})}

\subsubsection{Sample-efficiency in relational seq2seq tasks}

\textbf{Abstractor architecture} The Abstractor model uses architecture (b) of~\Cref{fig:abstractor_architectures}. For each of the Encoder, Abstractor, and Decoder modules, we use 2 layers, 2 attention heads/relation dimensions, a feedforward network with 64 units and an embedding/symbol dimension of 64. The Abstractor's symbols are learned parameters of the model, and uses $\sigma_{\mathrm{rel}} = \mathrm{Softmax}$. The number of trainable parameters is $386,954$.

\textbf{Transformer architecture} We implement the standard Transformer of~\citep{vaswani2017attention}. For both the Encoder and Decoder modules, we use 4 layers, 2 attention heads, a feedforward network with 64 units and an embedding dimension of 64. The number of trainable parameters is $469,898$. We increased the number of layers compared to the Abstractor in order to make it a comparable size in terms of parameter count. Previously, we experimented with identical hyperparameters (where the Transformer would have fewer parameters due to not having an Abstractor module).

\textbf{Ablation model architecture} The Ablation model uses an identical architecture to the Abstractor, except that the relational cross-attention is replaced with standard cross attention at the Encoder-Abstractor interface (with $Q \gets A, K \gets E, V \gets E$). It has the same number of parameters as the Abstractor-based model.

\textbf{Training/Evaluation} We use the sparse categorical crossentropy loss and the Adam optimizer with a learning rate of $10^{-3}$, $\beta_1 = 0.9, \beta_2 = 0.999, \varepsilon = 10^{-7}$. We use a batch size of 512. We train for 100 epochs and restore the best model according to validation loss. We evaluate learning curves by varying the training set size and sampling a random subset of the data at that size. Learning curves are evaluated starting at 100 samples up to 3000 samples in increments of 100 samples. We repeat each experiment 10 times and report the mean and standard error of the mean.

\subsubsection{Generalization to new object-sorting tasks}

\textbf{Abstractor architecture} The Abstractor model uses architecture (a) of~\Cref{fig:abstractor_architectures}. The Abstractor module has 1 layer, a symbol dimension of 64, a relation dimension of 4, a softmax relation activation, and a 64-unit feedforward network. The decoder also has 1 layer with 4-head MHA and a 64-unit feedforward network.

\textbf{Transformer architecture} The Transformer is identical to the previous section.

\textbf{Training/Evaluation} The loss, optimizer, batch size, and learning curve evaluation steps are identical to the previous sections. Two object-sorting datasets are created based on an ``attribute-product structure''---an primary dataset and a pre-training dataset. As described in~\Cref{ssec:experiments_object_sorting}, the pre-training dataset uses the same random objects as the primary dataset but with the order relation of the primary attribute reshuffled. The models are trained on 3,000 labeled sequences of the pre-training task and the weights are used to initialize training on the primary task. Learning curves are evaluated with and without pre-training for each model.

\subsection{Math Problem-Solving (\Cref{ssec:experiments_math})}

\textbf{Abstractor architecture} The Abstractor model uses architecture (d) of~\Cref{fig:abstractor_architectures}. The Encoder, ABstractor, and Decoder modules share the same hyperparameters: number of layers $L = 1$, relation dimension/number of heads $d_r = n_h = 4$, symbol dimension/model dimension $d_s = d_{\mathrm{model}} = 128$, projection dimension $d_p = 32$, feedforward hidden dimension $d_{\mathrm{ff}} = 256$. In the Abstractor, the relation activation function is $\sigma_{\mathrm{rel}} = \mathrm{softmax}$, and the symbols are nonparametric sinusoidal positional embeddings.


\textbf{Transformer architecture} The Transformer Encoder and Decoder have identical hyperparameters to the Encoder and Decoder of the Abstractor architecture.

\textbf{Transformer+ architecture} In `Transformer+', the model dimension is increased $d_\mathrm{model} = 200$ and the feedforward hidden dimension are increased to $d_{\mathrm{ff}} = 400$. The remaining hyperparameters are the same.

\textbf{Training/Evaluation} We train each model for 50 epochs with the categorical cross-entropy loss and the Adam optimizer using a learning rate of $6 \times 10^{-4}$, $\beta_1 = 0.9, \beta_2 = 0.995, \varepsilon = 10^{-9}$. We use a batch size of 128.