
\section{Function classes}
\label{sec:function_spaces}

\def\rdot{\bigcdot}
\def\F{{\mathfrak{F}}}

In this section we discuss function classes for relational learning and symbolic message passing.

\subsection{Robustness and error correction}

For the relational cross-attention mechanism used by abstrators, an $m\times m$ relation 
is computed as 
\begin{equation}
  R = \mbox{Softmax}(K^T Q)
\end{equation}
where $Q = W_Q E\in \reals^{d\times m}$ and $K = W_K E\in \reals^{d\times m}$ are the query and keys; the softmax is applied column-wise. So, each column $r_j \in \Delta_k$ is in the simplex of 
non-negative $k$-vectors summing to one. Let $(s_1,\ldots, s_m) = S\in\reals^{d\times m}$ be
a matrix of symbols. The relational cross attention then transforms the symbols by 
\begin{equation}
  A = SR
\end{equation}
so that each abstract variable $a_j$ is in the convex hull of the set of symbols.
As long as $S$ has rank $m$, relations are 
uniquely determined from the abstract symbols.

More generally, suppose that the symbols $S$ are transformed to $A$ and corrupted with additive noise:
\begin{equation}
  A = SR + \Xi
\end{equation}
where a fraction $\epsilon$ of the entries of $\Xi$ are drawn from an adversarial noise distribution, and the other entries are zero; dropout noise is also possible. 
This can be studied as an instance of compressed sensing and ``model repair'' \citep{candes_randall,model_repair}.  In particular, the relations can be recovered using the  robust regression estimator
\begin{equation}
  \hat r_j = \argmin_{u \in\reals^m} \| a_j - S u\|_1 \label{eq:lp}
\end{equation}
where $A = (a_1,a_2,\ldots, a_m)$ with columns $a_j\in\reals^d$.
The main lemma in \cite{model_repair} states that the following two conditions suffice:

\underline{Condition A:}
  There exists some $\sigma^2$, such that for any fixed $c_1,...,c_d$ satisfying $\max_i|c_i|\leq 1$,
  \begin{equation}
    \left\|\frac{1}{d}\sum_{i=1}^d c_i s_{i\rdot} \right\|^2\leq \frac{\sigma^2 m}{d},
  \end{equation}
with high probability, where $s_{i\rdot}\in\reals^m$ is the $i$th row of $S$.
  
\underline{Condition B:}
  There exist $\underline{\kappa}$ and $\overline{\kappa}$, such that
  \begin{eqnarray}
  \label{eq:l1-upper-A} \inf_{\|\Delta\|=1}\frac{1}{d}\sum_{i=1}^d|s_{i\rdot}^T\Delta| &\geq& \underline{\kappa}, \\
  \label{eq:l2-upper-A} \sup_{\|\Delta\|=1}\frac{1}{d}\sum_{i=1}^d|s_{i\rdot}^T\Delta|^2 &\leq& \overline{\kappa}^2,
  \end{eqnarray}
  with high probability.

\begin{thm}\label{thm:main-improved}
  Assume the symbol matrix $S$ satisfies Condition A and Condition B. Then if
  \begin{equation}
  \frac{\overline{\kappa}\sqrt{\frac{m}{d}\log\left(\frac{e d}{k}\right)}+\epsilon\sigma\sqrt{\frac{m}{d}}}{\underline{\kappa}(1-\epsilon)}
  \end{equation}
  is sufficiently small, the linear program \eqref{eq:lp} recovers $R$, so that $\hat r_j = r_j$ with high probability.
  \end{thm}

The condition is essentially that 
  \begin{equation}
    \frac{1}{1-\epsilon} \sqrt{\frac{m}{d}}
  \end{equation}
  is small, meaning that the dimension $d$ of the symbols needs to be sufficiently large relative 
  to the dimension $k$ of the relation.

  \subsection{Sparse, high-dimensional relations} 

 The above setting ensures enough redundancy to recover the relations, constraining the number of symbols $k$ to be small relative to the symbol dimension $d$. This is not appropriate in the situation where the relations are over a large number $m$ of elements, for example, the contents of the entire episodic memory.
 In this setting we assume that the relation tensor $R \in \reals^{m\times m}$ is sparse; that is, 
 each column $r_j \in \Delta_m$ has at most $k$ nonzero entries: $\|r_j\|_0 \leq k$. To recover the relation 
 we now use the robust lasso estimator, which is a related linear program
\begin{equation}
  \hat r_j = \argmin_{u \in\reals^m} \| a_j - S u\|_1 + \lambda\|u\|_1. \label{eq:rlasso}
\end{equation}  
Here we have an analogous theorem, stating that if 
\begin{eqnarray}
  \frac{\overline{\kappa}/\underline{\kappa}}{1-\epsilon}\sqrt{\frac{k}{d}\log(2m)}\leq c,
\end{eqnarray}
for some sufficiently small constant $c>0$, the robust lasso estimator \eqref{eq:rlasso} satisfies
\begin{equation}
  \|\hat r_j - r_j\| \leq C \frac{\overline{\kappa}/\underline{\kappa}^2}{1-\epsilon} \sqrt{\frac{\sigma^2 k}{d} \log(2m)}
\end{equation}
for some constant $C$.
This implies that we can accurately recover the relation tensor in the high dimensional setting, even when many of the entries of the transformed abstract symbols are corrupted.


The above discussion shows how the relation tensor can be recovered from the transformed symbols, even under adversarial noise, assuming there is sufficient redundancy in the symbols. This implies that it is possible to predict as well from the transformed symbols as from the relations, without explicitly recovering the relations. 



