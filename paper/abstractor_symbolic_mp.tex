
\newcommand{\MLP}{\text{MLP}}
\newcommand{\FeedForward}{\text{FeedForward}}
\newcommand{\Softmax}{\text{Softmax}}
%\newcommand{\reals}{\mathbb{R}}



% \begin{document}


\addtocounter{section}{1}
\section{Abstractors: Symbolic Message-Passing and Relational Cross-Attention}

\subsection{Relational Symbolic Message-Passing}
At a high level, the primary function of a relational abstractor is to compute abstract relational features of its inputs. That is, given a set of input objects $o_1, ..., o_n$, the relational abstractor computes a function on the set of pairwise relations between objects $\{ R(o_i, o_j) \}_{i,j}$, where $R(\cdot, \cdot)$ is a relation between a pair of objects.

At the core of relational abstractors is an operation we refer to as \textit{relational symbolic message-passing}. The input to this operation is a relation tensor $R = \left[R(o_i, o_j)\right]_{i,j=1}^n$, where $R(o_i, o_j) \in \mathbb{R}^{d_r}$ is a vector describing the relation between object $o_i$ and object $o_j$. We will come back to how an abstractor computes the relation tensor.

The first set of learnable parameters of symbolic message-passing is a set of symbols $s_1, ..., s_n \in \mathbb{R}^{d_s}$, where $d_s$ is the dimension of the symbolic vectors (a hyperparameter). We call these parameters \textit{symbols} because each of them `references' a particular object, but they are independent of the `values' of these objects. That is, the $i$th symbol `references' the $i$th object, but the value of $s_i$ is independent of the value of $o_i$. The use of those learned input-indpendent symbols is how symbolic message-passing achieves its abstraction.

In relational symbolic message-passing, we perform message-passing on these learned symbolic parameters according to the relation tensor $R$. In general, this message-passing operation can be described as a set-valued function of the form

\begin{equation}
    \label{eq:symbolic_message_passing}
    s_i' = \text{Update}\Big( s_i, \ \left\{ \left(R[i,j], s_j\right) \colon j=1, ..., n \right\}\Big), \quad i = 1, ..., n
\end{equation}

That is, the value of the $i$th symbol is updated as a function of the set of tuples $(R[i,j], s_j)$ of the relations with all other objects and the symbols of these objects. A simple but important special case of this is
\begin{equation}
    \label{eq:linear_symbolic_mp}
    s_i' = \sum_{j=1}^{n} R[i,j] s_j, \quad i=1, ..., n
\end{equation}

In the above, suppose that $d_r = 1$. Otherwise, the above operation is done for each dimension of the relation $R$ and the result is concatenated, as in multi-head attention.

Following message-passing, each updated symbol $s_i'$, can be passed through a feedforward neural network $f:\reals^{d_s}\rightarrow \reals^{d_s}$ to compute a non-linear function of the output. Empirically, a residual connection and layer normalization may be useful, as in a transformer.

This message-passing operation can be repeated multiple times to iteratively update the symbolic vectors. We can index the updated symbolic vectors at layer $l$ by $(s_1^{(l)}, ..., s_n^{(l)})$. The output of relational symbolic message-passing is the set of symbols at the end of this sequence of mesage-passing operations. The overall procedure is summarized in \ref{alg:relational_abstractor}. In \ref{sec:function_spaces} we characterize the class of functions on relations that this operation can compute.

\subsection{Computing the Relation Tensor via Relational Cross-Attention}

The above subsection describes the first of the two main ingredients of abstractors. What remains is to describe how the abstractor computes the relation tensor $R$. This is done through a variant of transformer cross-attention that we refer to as \textit{relational cross-attention}.

To motivate this, we first describe how we can compute `relations' between pairs of object through inner products. The inner product operation is a natural way to capture notions of `relations' and `similarity'. In euclidean space, inner products capture `alignment' between vectors. Similarly, for objects with vector representations, inner products between these vector representations can capture relations between these objects.

In general, we can formulate inner product relations as the standard euclidean inner product between a pair of transformed object vectors. That is, $R(o_i, o_j) = \langle \phi(o_i), \psi(o_j) \rangle$. This captures a large class of relation functions. In particular, it includes any function on pairs of objects which is continuous, symmetric, and positive semi-definite (by Mercer's theorem). Multi-dimensional relation functions can be achieved by stacking multiple such inner products.

Assuming that the vector representation of the objects is already rich enough, the transformations $\phi$ and $\psi$ can be limited to be linear transformations. We will suggestively call these linear transformations $W_Q$ and $W_K$, respectively. For the message-passing operation \Cref{eq:linear_symbolic_mp}, it is useful to normalize the relations $R$ via a softmax so that they are non-negative and sum to 1. Hence, after each symbolic message-passing operation, the updated representation of each symbol involves a convex combination of the other symbols, proportion to the relations between them.

This can be compactly written as

\begin{equation}
    \label{eq:relational_crossattention}
    \begin{split}
        \boldsymbol{s}' &= \boldsymbol{s} R, \\
        R &= \Softmax\left((W_Q \boldsymbol{o})^\top (W_K \boldsymbol{o})\right),
    \end{split}
\end{equation}

\noindent where $\boldsymbol{s} = \begin{bmatrix}s_1 & \cdots & s_n \end{bmatrix} \in \reals^{d_s \times n}$ is the matrix of symbols and $\boldsymbol{o} = \begin{bmatrix}o_1 & \cdots & o_n \end{bmatrix} \in \reals^{d_o \times n}$ is the matrix of input objects. This is essentially the cross-attention operation of transformers, where the `queries' and `keys' both come from the input objects, and the `values' come from the learned input-independent symbols. Hence, we refer to this operation as relational cross-attention and denote it by $\text{CrossAttention}(Q \gets \boldsymbol{o}, K \gets \boldsymbol{o}, V \gets \boldsymbol{s})$

\subsection{Abstractors}

With the two ingredients in place, we can formulate the general framework. The input to an abstractor is a set of entities $(z_1, ..., z_n)$. As described in the next section, we will see that abstractors can be integrated into a transformer-based model in a powerful way by having these entities come from a transformer encoder. At each layer, the abstractor updates the abstract symbolic vectors referencing the input objects via a series of self-attention, relational cross-attention, and feedforward operations. This is summarized in \ref{alg:relational_abstractor}.

\begin{algorithm}[ht!]
    \caption{Relational Abstractor}\label{alg:relational_abstractor}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \SetKwInOut{LearnableParams}{Learnable Parameters}
    \SetKwInOut{HyperParams}{Hyper Parameters}

    \Input{Encoder entites: $\boldsymbol{z} = (z_1, ..., z_n) \in \mathbb{R}^{d_e \times n}$}
    \HyperParams{$L$ (number of layers), $H$ (number of heads), hyperparameters of feedforward network}
    \LearnableParams{symbols $\boldsymbol{s} = (s_1, ..., s_n) \in \reals^{d_s \times n}$, parameters of multi-head self-attention/cross-attention, parameters of feedforward network}
    \Output{Abstracted sequence: $\boldsymbol{a} = (a_1, ..., a_n) \in \reals^{d_a \times n}$}
    \vspace{1em}

    $\boldsymbol{a} \gets \boldsymbol{s}$

    \For{$l \gets 1$ \KwTo $L$}{
        $\boldsymbol{a} \gets \text{SelfAttention}(\boldsymbol{a})$

        $\boldsymbol{a} \gets \text{CrossAttention}\left( Q \gets \boldsymbol{z}, K \gets \boldsymbol{z}, V \gets \boldsymbol{a} \right)$

        $\boldsymbol{a} \gets \FeedForward(\boldsymbol{a})$
        }
\end{algorithm}


% NOTE / TODO: need to describe what we mean by `relations', `relation functions', `inner product relations', etc. in more detail somewhere? 
% maybe in intro at high-level

% comment this out
% \end{document}
