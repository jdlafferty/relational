\begin{abstract}
    An extension of Transformers is proposed that enables explicit relational reasoning through a novel module called the \textit{Abstractor}. At the core of the Abstractor is a variant of attention called \textit{relational cross-attention}. The approach is motivated by an architectural inductive bias for relational learning called the ``relational bottleneck," which disentangles relational information from extraneous sensory information, to enable more focused and explicit relational reasoning, leading to abstraction and generalization from limited data.
    % [awni]: question. should we mention the relational bottleneck here? or we can just describe it: "The approach is motivated by an architectural inductive bias for relational learning which disentangles relational information from extraneous sensory information, to enable..."
    % In addition to evaluating the Abstractor on simple discriminative relational tasks and comparing to existing relational architectures, we also evaluate the Abstractor on relational sequence-to-sequence tasks, observing dramatic improvements in sample efficiency compared to a standard Transformer. We also evaluate the Abstractor on a collection of mathematics problems that benefit from a combination of relational reasoning and more traditional sequence modeling.
    We begin our empirical evaluation in a more controlled setting with a set of synthetic discriminative and generative relational tasks, where we compare to existing relational architectures and standard Transformers, observing dramatic improvements in sample-efficiency. Following this, we compare the Abstractor to a Transformer on a more realistic set of tasks based on mathematical problem-solving, and observe a modest but consistent improvement in performance and sample efficiency.
\end{abstract}

% A version of the Abstract I wrote simultaneously
% \begin{abstract}
%     Through simple attention mechanisms, the Transformer architecture supports general-purpose sequence-modeling and has the ability to learn a wide range of tasks.
%     However, this versatility comes at the cost of sample-efficiency in more specialized processing.
%     In particular, relational reasoning in Transformers emerges implicitly given enough data,
%     but the ``entangled'' representations produced by standard attention mechanisms result in suboptimal sample-efficiency for learning relations and limited generalization to new domains.
%     In this work, we propose an extension of the Transformer framework via a novel module which achieves enhanced relational processing by separating relation information from extraneous features about individual objects.
%     We empirically evaluate the proposed architecture on a range of tasks which require some form of relational reasoning and demonstrate superior sample-efficiency compared to standard Transformers.
% \end{abstract}

% The old abstract from the NeurIPS submission
% \begin{abstract}
%     Reasoning in terms of relations, analogies, and abstraction is a hallmark of human intelligence. An active debate is whether this relies on the use of symbolic processing or can be achieved using the same forms of function approximation that have been used for tasks such as image, audio, and, most recently, language processing.  We propose an intermediate approach, 
%     motivated by principles of cognitive neuroscience, in which abstract symbols can emerge from
%     distributed, neural representations under the influence of an inductive bias for learning that we refer to as a ``relational bottleneck.''  We present a framework that casts this inductive bias in terms of an extension of transformers, in which specific types of attention mechanisms enforce the relational bottleneck and transform distributed symbols to implement a form of relational reasoning and abstraction. We study the class of relation functions the models can compute and demonstrate superior sample-efficiency on relational tasks compared to standard transformer architectures.
%  \end{abstract}
