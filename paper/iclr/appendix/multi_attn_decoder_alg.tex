\section{Multi-Attention Decoder}\label{sec:multi_attn_decoder}

\begin{algorithm}[ht!]
	\caption{Multi-Attention Decoder}\label{alg:multi_attention_decoder}
	\SetKwInOut{Input}{Input}
	% \SetKwInOut{Output}{Output}
	% \SetKwInOut{LearnableParams}{Learnable parameters}
	% \SetKwInOut{HyperParams}{Hyperparameters}

	\Input{
        Target sequence: $\bm{y} = (y_0, \ldots, y_{l_y-1})$, \\
        Context sequences: $X^{(i)} = (x_1^{(i)}, \ldots, x_{l_i}^{(i)}), \ i=1, \ldots, K$
        }
	\vspace{1em}

    $D^{(0)} \gets \bm{y}$

	\For{\(l \gets 1\) \KwTo \(L\)}{

        $D^{(l)} \gets \mathrm{CausalSelfAttention}\paren{D^{(l-1)}}$

        \texttt{residual connection and layer-normalization}


        \For{\(i \gets 1\) \KwTo \(K\)}{
            $D^{(l)} \gets \mathrm{CrossAttention}\paren{D^{(l)}, X^{(i)}}$

            \texttt{residual connection and layer-normalization}
        }

        $D^{(l)} \gets \mathrm{FeedForward}\paren{D^{(l)}}$

        }

    \textbf{Output:} $D^{(L)}$

\end{algorithm}
