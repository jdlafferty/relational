\subsection{Related Work}\label{ssec:related_work}
A growing body of literature is focused on developing machine learning architectures with explicit relational reasoning capabilities. An early example is the Relation Network proposed in~\citep{santoro1}. The essential idea here is process pairwise relations by applying an MLP to the concatenation of object representations and aggregating the outputs by a simple summation. Given a sequence of objects $X = (x_1, \ldots, x_\m)$ as input, the Relation Network is given by $\mathrm{RN}(X) = f_\phi(\sum_{ij} g_\theta(x_i, x_j))$, where $f_\phi, g_\theta$ are MLPs.~\citep{shanahanExplicitlyRelationalNeural} proposes the PrediNet architecture which aims to learn representations of relations which are inspired by predicate logic. The ESBN model proposed in~\citep{esbn} is a memory-augmented LSTM network, inspired by ideas from cognitive science, which aims to factor representations into `sensory' and `relational'. In this sense, it is similar in spirit to the present work. Another architecture which is similar in spirit is the CoRelNet architecture proposed in~\citep{kerg2022neural}, which reduces relational learning to modeling a similarity matrix. It is given by, $\mathrm{MLP}(\mathrm{flatten}(R))$, where $R$ is the similarity matrix $R = \mathrm{Softmax}\paren{[\iprod{\phi(x_i)}{\phi(x_j)}]_{ij}}$ and $\phi$ is an encoder.

The Transformer~\citep{vaswani2017attention} is a common baseline which is compared against in this literature~. It is shown in these works that explicitly relational architectures outperform the Transformer, sometimes by large margins, on several synthetic discriminative relational tasks~\citep{shanahanExplicitlyRelationalNeural,esbn,kerg2022neural}. In this work, we offer an explanation, arguing that while the Transformer architecture is versatile enough to learn such relational tasks given enough data, it does not support relational reasoning explicitly. The Abstractor module extends the Transformer framework by learning representations of relations which are disentangled from extraneous features about individual objects. Our experiments first validate that the Abstractor, on its own, achieves the same sample-efficiency gains of other relational architectures on discriminative relational task. We then evaluate whether the Abstractor can augment a Transformer to improve relational reasoning by evaluating on synthetic \textit{sequence-to-sequence} relational tasks, which has so far been unexplored in the literature on explicitly relational architectures. Finally, we evaluate an Abstractor-based architecture on a more realistic mathematical problem-solving task to evaluate the potential of the idea on more general tasks.