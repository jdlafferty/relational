\subsection{Related Work}

There exists an emerging literature on developing machine learning architectures with explicit relational reasoning capabilities. An early example is the Relation Network proposed in~\citep{santoro1}. The essential idea here is process pairwise relations by applying an MLP to the concatenation of object representations and aggregating the outputs by a simple summation. Given a sequence of objects $X = (x_1, \ldots, x_\m)$ as input, the Relation Network is given by $\mathrm{RN}(X) = f_\phi(\sum_{ij} g_\theta(x_i, x_j))$, where $f_\phi, g_\theta$ are MLPs.~\citep{shanahanExplicitlyRelationalNeural} proposes the PrediNet architecture which aims to learn propositional representations. The ESBN model proposed in~\citep{esbn} is an architecture which is similar in spirit to the present work. It separates sensory information from relational information and processes the relational information using an LSTM controller. Another architecture which is similar in spirit is the CoRelNet architecture proposed in~\citep{kerg2022neural}, which reduces relational learning to modeling a similarity matrix. It is given by, $\mathrm{MLP}(\mathrm{flatten}(R))$, where $R$ is the similarity matrix $R = \mathrm{Softmax}\paren{[\iprod{\phi(x_i)}{\phi(x_j)}]_{ij}}$ and $\phi$ is an encoder.

Prior work has made great strides in relational learning and has demonstrated that architectures with the right inductive biases can result in significant data-efficiency improvements for learning relational tasks. The Abstractor makes several important contributions to this line of work. First, the relational cross-attention mechanism of the Abstractor provably captures arbitrary relation functions (see~\Cref{ssec:function_classes_preview}). In particular, it can model multi-dimensional and asymmetric relations. Moreover, existing work on relational architectures has focused on \textit{discriminative} relational tasks. The Abstractor framework naturally supports \textit{generative} sequence-to-sequence relational tasks. Finally, since the Abstractor framework is an extension of Transformers, it immediately inherits the strengths of Transformers in sequence-modeling. This enables the Abstractor to yield improvements in more general sequence-modeling tasks, such as natural language understanding, in addition to relational tasks.

In addition to comparing against existing architectures on discriminative relational tasks, we also compare against standrad Transformers on sequence-to-sequence relational tasks.