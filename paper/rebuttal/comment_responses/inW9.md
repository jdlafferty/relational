Thank you for your thoughts and comments. We appreciate your efforts to engage with our work.

> if the sorting task required outputting the original objects in their sorted locations rather than the indices, further architectural modification would be needed so that the Abstractor could express a solution.

That’s right, since it would require sensory information about the individual objects’ representations. In particular, the sensory-connected architecture which we propose, wherein the decoder attends to both the Encoder and the Abstractor, would be the natural candidate.

> The Abstractor is like a modification of transformers to have a strict inductive bias towards learning relational features. This dramatically improves sample efficiency for relational tasks but also cripples the model's ability to represent more complex behaviors without further architectural modification.

This is correct. But the end of the second sentence partially misses the point. The “Abstractor” is a *module* which fits in a larger transformer-based model architecture (here, we are using “Abstractor” to refer only to the module; in the paper we also use “Abstractor” to refer to models which contain an Abstractor module). The only purpose of the Abstractor module is to process relational information. It implements a relational bottleneck because this is a useful inductive bias which enables dramatic improvements in the sample-efficiency for relational processing. The architecture of the overall model should reflect the task. In a fully-relational task, the Encoder → Abstractor → Decoder architecture is the natural choice, and this is what we use for the argsort experiments because that is a fully-relational task. In a partially relational task, a natural architecture is the sensory-connected architecture we propose, wherein the decoder attends to both the encoder and the abstractor.

The abstractor is one module within a larger model architecture whose job is to process relational information. It does this well. It would be wrong to blame it for the overall model’s failure to do a particular task if that model architecture was chosen incorrectly. The Abstractor module’s inability to process non-relational information is not a limitation of its design—it’s not intended to do that.

We don’t present any experiments using the sensory-connected architecture, and we agree that such experiments would have been interesting to see. They would have been part of the submitted paper, given more time. But, since the primary promise of the Abstractor is that it can improve relational processing in a transformer-based architecture, we think that the first set of experiments involving it should isolate for this facet and validate whether it in fact achieves that—hence, the purely relational experiments. Future experiments can and should introduce additional complexities to validate whether something like the sensory-connected architecture yields improvements on partially-relational tasks.

We believe that this is a point that we can convey better in the final version of the paper.

**A comment on experiments**

Finally, we would like to briefly argue in favor of the sorting experiments as an interesting demonstration of the Abstractor’s abilities. Those experiments are synthetic, but we think sorting is an importing example of a relational task. The experiments demonstrate a significant improvement in sample efficiency.

On the other hand, with regard to comparison to other architectures, the experiments in the CorelNet/ESBN/PrediNet papers are also synthetically generated and they are hardly “real-world” tasks. They involve images, which one might view as an advantage, but the underlying relations which need to modeled to solve these tasks are often quite simple compared to the sorting experiments—typically same/different (such relations become even easier to model when temporal context normalization is used with a symmetry inductive bias. As mentioned in the CorelNet paper, the parameters of the encoder need not even be learnable since the inner products will always be large if the objects are the same and small otherwise). However, those experiments were still interesting demonstrations of those models’ abilities. Similarly, we think that the sorting experiments offer a compelling demonstration of the Abstractor’s abilities.

The sorting experiments introduce a novel kind of relational task into the literature which is *sequence-to-sequence* and has an underlying relation which is *asymmetric*. This is a task which previous relational architectures could not do, even in principle. We believe that when introducing a new kind of architecture, synthetic tasks can evaluate and isolate for more targeted properties than a more natural task could. More experiments and more comparisons is always a good thing, but there is a limit to what one paper can fit, and we believe the sorting experiments which our paper focuses on introduce something new to the literature of relational architectures and offer a compelling view of the Abstractor’s abilities and promise.