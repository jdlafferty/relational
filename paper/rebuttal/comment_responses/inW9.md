Thank you for your thoughts and comments. We appreciate your efforts to engage with our work.

> if the sorting task required outputting the original objects in their sorted locations rather than the indices, further architectural modification would be needed so that the Abstractor could express a solution.

That's correct, since it would require sensory information about the individual objects' representations. In particular, the sensory-connected architecture which we propose, wherein the decoder attends to both the Encoder and the Abstractor, would be the natural candidate. It is straightforward to extend our experiments to include this.

> The Abstractor is like a modification of transformers to have a strict inductive bias towards learning relational features. This dramatically improves sample efficiency for relational tasks but also cripples the model's ability to represent more complex behaviors without further architectural modification.

This is correct. But the end of the second sentence partially misses the point. The “Abstractor” is a *module* which fits into a larger transformer-based model architecture. (Here, we are using “Abstractor” to refer only to the module; in the same way, 
Encoders and Decoders are modules used to transform representations. In the paper we also use “Abstractor” to refer to models that contain an Abstractor module.) The purpose of the Abstractor module is to process relational information. Our results show how Abstractors enable dramatic improvements over standard Transformers in the sample-efficiency for relational processing. Of course, the architecture of the overall model should reflect the task. In a fully-relational task, the Encoder → Abstractor → Decoder architecture is the natural choice, and this is what we use for the argsort experiments because that is a fully-relational task. In a partially relational task, a natural architecture is the sensory-connected architecture we propose, wherein the Decoder attends to both the Encoder and the Abstractor.

The Abstractor is one module within a larger model architecture whose job is to process relational information. It does this well. It would not be appropriate to blame it for the overall model's failure to do a particular task if the model architecture was chosen incorrectly. The Abstractor module's inability to process non-relational information is not a limitation of its design—it's not intended to do that.

We don't present any experiments using the sensory-connected architecture, and we agree that such experiments would have been interesting to see. They would have been part of the submitted paper, given more time and space. But, since the primary promise of the Abstractor is that it can improve relational processing in a transformer-based architecture, we think that the first set of experiments involving it should isolate for this facet and validate whether it in fact achieves that—hence, the purely relational experiments. Future experiments can and should introduce additional complexity to validate whether something like the sensory-connected architecture yields improvements on partially-relational tasks.

We believe that this is a point that we can convey better in the final version of the paper.

**A comment on experiments**

Finally, we would like to briefly argue in favor of the sorting experiments as an interesting demonstration of the Abstractor's abilities. The experiments are synthetic, but we believe sorting is an important example of a relational task. The experiments demonstrate a remarkable improvement in sample efficiency, with essentially zero-shot learning for sorting a new set of objects with a pre-trained ordering relation.

[comment]: <> (On the other hand, with regard to comparison to other architectures, the experiments in the CorelNet/ESBN/PrediNet papers are also synthetically generated and are hardly “real-world” tasks. They involve images, which one might view as an advantage, but the underlying relations which need to modeled to solve these tasks are often quite simple compared to the sorting experiments—typically same/different such relations become even easier to model when temporal context normalization is used with a symmetry inductive bias. As mentioned in the CorelNet paper, the parameters of the encoder need not even be learnable since the inner products will always be large if the objects are the same and small otherwise. However, those experiments were still interesting demonstrations of those models' abilities. Similarly, we think that the sorting experiments offer a compelling demonstration of the Abstractor's abilities.)


The sorting experiments introduce a novel kind of relational task into the literature which is *sequence-to-sequence* and has an underlying relation which is *asymmetric*. This is a task which previous relational architectures could not do, even in principle. We believe that when introducing a new kind of architecture, synthetic tasks can evaluate and isolate for more targeted properties than a more natural task could. The SET experiments involve complex relational processing, trained end-to-end on images of cards. To be sure, more experiments and comparisons are always a good thing, but there is a limit to what one paper can fit, and we believe the sorting experiments which our paper focuses on introduce something new to the literature of relational architectures and offer a compelling view of the Abstractor's abilities and promise.