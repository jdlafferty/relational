Thank you for your questions.

(A)

That's right, the number of symbols is the same as the number of objects in the input sequence. We agree that it would be good to spell this out, and will do so in the final version of the paper (along with the revamping of section 2).

(B)

There are no constraints on the decoder. The decoder simply attends to the objects at the ouptput of the Abstractor via standard cross-attention (in the sensory-connected architecture, the decoder also attends to the encoder). This should be thought of as the decoder attending to the relational information extracted by the Abstractor in order to perform the task (in the same way that in standard transformers a decoder attends to the information represented by the encoder).

In our experiments, the decoder is indeed autoregressive. The target is to predict the *argsort*. That is, the input to the overall model is a sequence of objects, each of which is represented by a vector. The output sequence is the indices of these objects in sorted order. For example, suppose the input contains 10 objects. Then the output at each position is a softmax vector of length 10. The decoder is autoregressive in the sense that it takes as input a right-shifted version of the target output (the argsort) and transforms it causally to a prediction of the target (the argsort).

(C)

This is a good question. Indeed, the symbols S need not be learned. The theory suggests that as long as the symbols are well-separated (in particular, linearly independent), the function class remains the same. When the symbols are initialized randomly, they will in fact be roughly orthogonal in high-dimensions.

In the experiments we report in the paper, the symbols are learned. But we also ran experiments in which the symbols were initialized randomly and fixed. We indeed observe that this does not hurt performance. One option, is to fix the the symbols to be sinusoidal positional embeddings. We have also tried this and this works as well (since they are also well-separated). However, we did not experiment with whether this allows the model to directly generalize to longer sequences.

One relevant experiment is in the supplementary material, but did not make it into the paper (under `code/experiments/sorting_w_relation_prelearning`; see the `readme.md` and `learning_curve_analysis.ipynb` if interested). In this experiment, we train an abstractor to learn the $\prec$ relation on pairs of objects, then use the weights learned on this task to initialize an autoregressive abstractor model to perform argsort on a sequence of 10 objects (i.e., the W_k, W_q, W_v weights matrices, etc. the symbols are initialized randomly since there is a different number of symbols). We observe that this pre-training on the pairwise task improve learning curves on the full 10-object sorting task.

(D)

As described, with the learned symbols, the architecture supports a fixed-size input. However, as discussed above, an Abstractor can use fixed positional embeddings as symbols. This does not hurt performance compared to learned symbols on a fixed-size task. We did not test whether this enables generalization to longer sequences.

It is relevant to point out that standard transformers have been known to fail to generalize to longer sequences under this set up (see for e.g., [“The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers” by Csordas et. al.], and the references therein). One approach which helps with generalization to longer sequences is using relative positional embeddings. In the context of symbolic message-passing, this would amount to using position-relative symbols. That is,

$$
a_i = \sum_j R[i,j] s_{j-i} 
$$

where the learned parameters are now $S = (s_{-m+1}, \ldots, s_{-1}, s_0, s_1, \ldots, s_{m-1})$. This results in representing information about object’s relations with the other objects in the sequence in a ‘centered’ coordinate system. Thus, abstractors have the same strengths and limitations as standard transformers with respect to generalization to longer sequences, and the same approaches can be employed to remedy this issue.

We think that position-relative symbols are an interesting variant of the Abstractor for this as well as other reasons (although it comes at computational costs). We will add a brief discussion to the paper about the challenge of generalization to longer sequences and the position-relative symbols variant of the Abstractor.