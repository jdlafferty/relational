> Is my understanding of the Abstractor architecture correct?

We appreciate your feedback on the presentation and structure of the paper. We generally agree, and will revamp the description of the architecture, especially in section 2.

The description you give of the Abstractor architecture is not correct. Please see the global response (and the description below) for a clearer description of the Abstractor architecture and how relational cross-attention differs from standard self-attention in a Transformer.

The core difference between a Transformer encoder and an Abstractor is seen in the difference between self-attention and relational cross-attention. Let $X = (x_1, \ldots, x_m)$ be the input sequence. In a transformer, self-attention transforms the input sequence via a message-passing operation of the form
$$
\begin{align*}
    E_i &\gets \sum_j \alpha_{ij} \phi_v(x_j) \\
    (\alpha_{ij})_{j=1}^m &\gets \mathrm{Softmax}\left(\left[\langle \phi_q(x_i), \phi_k(x_j)\rangle\right]_{j=1}^m\right),
\end{align*}
$$

where $\phi_q, \phi_k, \phi_v$ are the query, key, and value maps, respectively (typically linear in a transformer). This can be interpreted as a message-passing operation where the message that object $j$ sends object $i$ is (a function $\phi_v$ of) its own value, weighted by the relation between them. This mixes relational information with the value-information of individual objects. It does not directly model the relations between objects. 

Self-attention, of course corresponds to $\mathrm{Attention}(Q \gets X, K \gets X, V \gets X)$. The crucial modification that relational cross-attention makes is that, instead of the messages being the values of the objects, they are now *input-independent symbols*. That is, we learn a set of parameters $S = (s_1, \ldots, s_m)$ which identify the objects in the sequence, and send those symbols as messages, weighted by the relations between objects. That is,
$$
\begin{align*}
    A_i &\gets \sum_j R[i,j] s_j \\
    R &\gets \sigma\left(\left[\langle \phi_1(x_i), \phi_2(x_j)\rangle\right]_{i,j=1}^m\right),
\end{align*}
$$

where $\phi_1, \phi_2$ are the left and right encoders (this corresponds to the query/key mappings), $s_j$ are the learned input-independent symbols, and $\sigma$ is the relation activation function. If $\sigma = \mathrm{Softmax}$, this operation corresponds to $\mathrm{Attention}(Q \gets X, K \gets X, V \gets S)$, where $S$ are learned input-independent parameters. Since the messages are input-independent symbols, relational cross-attention represents only relational information. This separation of relational information from the value information of individual objects is what we refer to as the *relational bottleneck*.

The idea of separating relational information from sensory information appears in prior work proposing relational architectures (CorelNet, ESBN, etc.). The innovation of this work is to extend this idea to the transformer framework through the abstractor and relational cross-attention. "Symbolic relational message-passing" is the term we give the message-passing interpretation of this operation and how it computes relational functions.

As the above hopefully clarifies, the Abstractor is not a variation of the universal Transformer. While, indeed, the attention matrix in the Abstractor (relation matrix/tensor), is computed with the same set of objects at each layer, this is not core difference. What implements the relational bottleneck is that the 'values' of attention ('messages' under the message-passing interpretation) are learned input-independent symbols.

We agree that the relationship to existing architectures should be spelled out more clearly. In addition to revamping the description of the architecture to make it more concise, we will also add a discussion on how it contrasts to existing architectures, particularly to standard transformers or universal transformers.

Please let us know if this clarifies the architecture and if you have any further questions or comments.

> The experiments are relatively unimpressive. The authors show that the abstractor can sort objects. The authors state that improvements on sorting is due to a "relational bottleneck", but it does not look like a bottleneck to me. I

Hopefully the above clarifies how the relational bottleneck arises in the Abstractor. The theory on function classes in section 3 of the main paper and section B of the supplement should further clarify this.

> Unfortunately, the authors are not clear about how many iterations the abstractor runs; I assume it runs for more than 1, otherwise it would just be a transformer. This is a crucial piece of information that should have been supplied.

The exact hyperparameters are provided in the supplement (section C). The Abstractor model is 2-layers deep. The transformer model reported in the experiments is 4-layers deep (we increase the number of layers over the Abstractor model so that it has a comparable parameter count). We included full code with the submission. We also kept detailed logs on `wandb.ai` which would be made publicly available after the double-blind process. We agree that the exact architectures/hyperparameters being compared is relevant information which should be in the main paper. We will add the relevant architectural details to the main paper.

> Section 4.5 (the SET problem), describes an experiment, but does not provide the results of the experiment.

The results are in Figure 2b. We forgot to reference the figure in the text. We have fixed this mistake.
