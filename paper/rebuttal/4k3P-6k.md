> It is mentioned in the main body as well as in the appendix that the symbolic message-passing can be applied several times. Will there be an observation of over-smoothing effect if applied too many times?

Similar, to message-passing in GNNs, over-smoothing can result if too many iterations of message-passing are applied. Theorem 1 of [“Representation Learning on Graphs with Jumping Knowledge Networks”—Xu et. al.] provides one analysis of over-smoothing in message-passing neural networks. 

The relational cross-attention operation corresponds to one step of message-passing in a single Abstractor layer. Adding additional Abstractor layers carries out relational cross-attention on the previously transformed abstract symbols.

> In line 103, what does the "may be useful to normalize the relations R via a soft-max" mean? Please provide more details.

This comment is meant as a brief discussion of the effects of applying an activation function to the relation tensor. We will expand on this point in the final version of the paper. We give a brief explanation here. Let $X = (x_1, \ldots, x_m) \in \mathcal{X}^m$ be input sequence of objects. The relation tensor computes a $d_r$-dimensional vector of relations between each pair of objects $i,j \in [m]$. The $k \in [d_r]$ component of the relation vectors is given by the $m \times m$ matrix
$$
R_k = \sigma\left( \phi_1^{(k)}(X)^\top \phi_2^{(k)}(X)\right)
$$
where $\phi_1^{(k)}(X) = [\phi_1^{(k)}(x_1) \ \cdots  \ \phi_1^{(k)}(x_m)]$. Applying an activation function $\sigma$ to the relation matrix has empirical benefits. For instance if $\sigma$  is the tanh or sigmoid function, it acts as a normalization to a fixed range. If $\sigma$ is the softmax function, then rather than acting elementwise, it normalizes with respect to the context of the other relations involving each object.

> In Eq. (2.4), the vectors fed to Q,K,V are changed to E,E,S, respectively. What is the benefit of doing this? How could it be if the vectors are E, S, S, respectively?

Attention is an operation which takes three inputs, corresponding to queries, keys, and values. So, the notation $\mathrm{Attention}(Q \gets \cdot, K \gets \cdot, V \gets \cdot)$ simply means,
$$
\mathrm{Attention}(Q \gets X, \ K \gets Y, \ V \gets Z) = W_v Z \  \mathrm{Softmax}((W_q X)^\top (W_k Y))
$$
where $W_q, W_k, W_v$ are the parameters of the attention operation. The notation $Q \gets \cdot$ should be read as “the queries in the attention operation come from $\cdot$ ”. Equation 2.4 is saying that, in an architecture like the one shown in Figure 1 of the main paper where the input to the Abstractor is the encoder states $E$, relational symbolic message-passing corresponds to the attention operation $\mathrm{Attention}(Q \gets E, \ K \gets E, \ V \gets S)$, where $S$ are the symbols which are trainable parameters.

> I noticed that "relational bottleneck" is a central idea / support of this work. Can you formalize it? It would be good if it is discussed after its very first appearance.

The “relational bottleneck” is indeed a central idea of this work. The idea of the relational bottleneck is to separate representations of the values of individual objects from representations of relations among objects. Standard self-attention in transformers encodes information about both relations as well as value information of individual objects. The way that Abstractors and relational cross-attention achieve the relational bottleneck is that the values are *input-independent symbols,* which are weighted by the relations between objects. Hence, the Abstractor encodes purely relational information.

> In line 133, what are lower level relations and higher order relations? Can you provide any examples?

We will edit the paper to add an explanation of what precisely we mean by higher-order relations and provide some examples. This is explained in the corresponding theory within the supplement. A one-layer abstractor returns a sequence of objects $A = (a_1, \ldots, a_m)$ where each $a_i$ encodes information about the relations of $i$ with all other objects. These are ‘relational objects’. Higher-order relations are relations between relational objects. This is what would be computed when we pass $A$ as input into another Abstractor.

Concretely, consider as a simple example a relational match-to-sample task. You are given a sequence of four objects. The first two objects have a relation between them and the second two objects have a relation between them (the relations are typically same/different). The task is to determine whether the relation between the first pair of objects is the same as the relation between the second pair of objects. This is an example of a second-order relation.  The Abstractor framework can model higher-order relations by composing Abstractor modules.

> In line 154, what are relational sequence-to-sequence tasks?

A relational sequence-to-sequence task is a sequence-to-sequence task which relies on processing relational information between objects in the input sequence in order to generate the correct output sequence. Sequence-to-sequence tasks that rely on relational information are widespread; derivations of expressions in mathematics provide a rich class of examples (take Bayes’ rule as a simple example).

This will be explained more clearly in the final version of the paper.

> In line 170, the term "chaining together abstractors" may lead to confusion. Does it mean "abstract layers" in line 132?

“Chaining together abstractors” means composing several abstractor modules together. For example, as depicted in Figure 3 of the page of figures.

We will edit the paper to clarify what “composing Abstractors” means and to distinguish between abstractor “layers” and “modules”.

> In line 184, the appearance of $\mathcal{X}$ is quite sudden. I managed to get what it refers to in appendix, but it would be better if it is described in the main text.

Agreed. Thanks for the feedback.

> As shown in Fig. 2(c), Abstractor requires much less data for training. What mechanism/design in Abstractor leads to this observation?

We attribute the dramatic improvement in sample efficiency on relational tasks to the relational bottleneck within the Abstractor. The sorting task is a “purely relational” task in the sense that producing the “argsort” of any sequence of objects can be done by processing the pairwise $\prec$ relation. No further information about the values of individual objects is necessary. Hence, the relational bottleneck forms a useful inductive bias.

> What are the pre-training tasks mentioned in line 285? Please provide more details.

The pre-training task is described in the previous paragraph (lines 278-282). The experiment in this section builds on the experiment described in the previous section. 

> I would like to see the training time, learning-rate, and devices used for training.

This will be added to the paper. The devices used are already there. We have also kept complete and detailed logs of all experiments (via wandb.ai) which are linked on the project repo. This was not included in the submission for anonymity but will be made public after the double-blind process. The logs include every trial of all experiments, both those that made it into the paper and not. The logs include metrics tracked by epoch, devices used, training time, code, system utilization, etc.