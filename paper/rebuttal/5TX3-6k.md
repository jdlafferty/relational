> Is my understanding of the Abstractor architecture correct?

We appreciate your feedback on the presentation and structure of the paper. We generally agree, and will revamp the description of the architecture, especially in section 2.

The description you give of the Abstractor architecture is not correct. Please see the global response (and the description below) for a clearer description of the Abstractor architecture and how relational cross-attention differs from standard self-attention in a Transformer.

The core difference between a Transformer encoder and an Abstractor is seen in the difference between self-attention and relational cross-attention. Let $X = (x_1, \ldots, x_m)$ be the input sequence. In a transformer, self-attention transforms the input sequence via a message-passing operation of the form
$$
\begin{align*}
    E_i &\gets \sum_j \alpha_{ij} \phi_v(x_j) \\
    (\alpha_{ij})_{j=1}^m &\gets \mathrm{Softmax}\left(\left[\langle \phi_q(x_i), \phi_k(x_j)\rangle\right]_{j=1}^m\right),
\end{align*}
$$

where $\phi_q, \phi_k, \phi_v$ are the query, key, and value maps, respectively (typically linear in a transformer). This mixes relational information with the value-information of individual objects. It does not directly model the relations between objects. 

Self-attention, of course corresponds to $\mathrm{Attention}(Q \gets X, K \gets X, V \gets X)$. The crucial modification that relational cross-attention makes is that, instead of the values being functions of the input objects, they are now *input-independent symbols*. That is, we learn a set of parameters $S = (s_1, \ldots, s_m)$ which ``are bound to'' the objects in the sequence, and use those as the values.
This operation corresponds to $\mathrm{Attention}(Q \gets X, K \gets X, V \gets S)$, where $S$ are learned input-independent vectors. This separation of relational information from the value information of individual objects' sensory encoding is what we refer to as the *relational bottleneck*.

The idea of separating relational information from sensory information appears in prior work proposing relational architectures (CorelNet, ESBN, etc.). The innovation of our  work is to show how this can be done in the transformer framework through the abstractor and relational cross-attention. 

As the above hopefully clarifies, the Abstractor is not a variation of the universal Transformer. While, indeed, the attention matrix in the Abstractor (relation matrix/tensor), is computed with the same set of objects at each layer, this is not core difference. What implements the relational bottleneck is that the 'values' of attention are learned input-independent symbols.

We agree that the relationship to existing architectures should be spelled out more clearly. In addition to revamping the description of the architecture to make it more concise, we will also add a discussion on how it contrasts to existing architectures, particularly to standard transformers or universal transformers.

Please let us know if this clarifies the architecture and if you have any further questions or comments.

> The experiments are relatively unimpressive. The authors show that the abstractor can sort objects. The authors state that improvements on sorting is due to a "relational bottleneck", but it does not look like a bottleneck to me. I

Hopefully the above clarifies how the relational bottleneck arises in the Abstractor. The theory on function classes in section 3 of the main paper and section B of the supplement should further clarify this.

> Unfortunately, the authors are not clear about how many iterations the abstractor runs; I assume it runs for more than 1, otherwise it would just be a transformer. This is a crucial piece of information that should have been supplied.

Our implementation defines a new attention mechanism; this corresponds to a single symbolic message passing step in a given layer. The Abstractor model is 2-layers deep. The transformer model reported in the experiments is 4-layers deep (we increase the number of layers over the Abstractor model so that it has a comparable parameter count). We included full code with the submission. We also kept detailed logs on `wandb.ai` which will be made publicly available after the double-blind process. We will add the relevant architectural details to the main paper.

> Section 4.5 (the SET problem), describes an experiment, but does not provide the results of the experiment.

The results are in Figure 2b; we neglected to reference the figure in the text, and have fixed this mistake.
