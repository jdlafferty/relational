\section{Function classes}
\label{ssec:function_classes_preview}

In this section, we will discuss the class of relational functions computable by Abstractors by characterizing the multi-head relation and symbolic message-passing operations. We also comment on the robustness of these operations. These results are important for appreciating the expressivity of relations and symbolic message passing, but are more technical and we therefore gather the results in the supplement where we state the setting and assumptions more precisely. A summary of the results follows.

\def\rdot{\bigcdot}
\def\F{{\mathfrak{F}}}
\def\MLP{\text{MLP}}

\subsection{Universal approximation for inner product relations}

Our first result is a universal approximation result for inner product relations. This is useful when characterizing the class of functions computable by the multi-head relation module within abstractors, but is also of independent interest more generally for relational machine learning.
We would like to learn a relation function \(r: \mathcal{X} \times \mathcal{X} \to \reals^{d_r}\) which maps pairs of objects in \(\mathcal{X}\) to a \(d_r\)-dimensional vector describing the relation between these objects. We model this relation function as a vector of inner products between transformations of the objects' representations,
using learnable transformations corresponding to each dimension of the relation, thought of as \textit{relational filters}. They extract a particular attribute of the objects such that an inner product of the transformed objects indicates the alignment or similarity along this attribute. Having several different filters allows for modeling rich multi-dimensional relations. This is one notable advantage of this formulation over the CoRelNet model \citep{kerg2022neural}, which processes a relation matrix as input to a multi-layer perceptron. Conisder first the symmetric version of~\Cref{eq:multi_head_rel} with $\phi_i = \psi_i$ as multi-layer perceptrons.

\begin{result}
    Symmetric inner product relational neural networks are equivalent to Mercer kernels, in the sense that any symmetric multi-dimensional relation forming a Mercer kernel can be uniformly approximated in the $\sup$-norm by inner products between multi-layer perceptrons $\phi_i$.
\end{result}

This is precisely stated in Theorem~\ref{theorem:function_class_inner_product_relnn}. 
Additionally, we can consider inner products of the form
$\langle W_k^{(1)} \phi(x_i), W_k^{(2)} \phi(x_j) \rangle$
where the linear projections for the first and second entities may be different and $\phi$ is a common non-linear encoder. This models non-symmetric relation functions and yields a strictly larger class of relation functions.

\subsection{Class of relational functions computable by symbolic message-passing}

Our next result characterizes the class of relational functions computable by symbolic message-passing.
Specifically, for appropriately chosen distributed symbols, 
a one-step symbolic message-passing operation can compute arbitrary functions of each object's relations with other objects in the input sequence. 

\begin{result}
    There exists a choice of symbols \(s_1, \ldots, s _\m\) and parameters of feed-forward networks such that abstract symbols computed by symbolic message passing compute an arbitrary function of each object's relations.
\end{result}

This result is stated formally as \Cref{lemma:function_class_1_step_symbolic_mp}.

\subsection{Composing  abstractors to compute relations on relations}
\label{ssec:compsing_abstractors_preview}

Next, we consider, the function class generated by a composition of several abstractors. In this case we see that chaining together a sequence of  abstractors allows the computation of relations on relations.

\begin{result}
A chain of \(k\) single-layer  abstractors is able to compute arbitrary \(k\)th order relational functions.
\end{result}

\subsection{Robustness and error correction}

For the relational cross-attention mechanism used by abstractors, an \(m\times m\) relation \(R\) is computed using a multi-head relation module and symbolic message-passing then transforms the symbols by
\(A = SR\). %so that each abstract variable \(a_j\) is in the convex hull of the set of symbols.
As long as \(S\) has rank \(m\), relations are uniquely determined from the abstract symbols. Here we observe how the transformed symbols can be robust to noise if the symbols are sufficiently redundant.

\begin{result}
Suppose that symbolic message-passing is used to transform a sequence of $m$ symbols, each of dimension $d$. If a fraction $\epsilon$ of the entries of the transformed symbols are arbitrarily corrupted, the relations can be exactly recovered by a linear program as long as $\sqrt{m/(1-\epsilon)^2d}$ is sufficiently small.
\end{result}

The results of~\cite{model_repair} make the robustness properties precise. We investigate different forms of robustness empirically in the experiments section.
