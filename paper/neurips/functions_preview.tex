\section{Function classes}
\label{ssec:function_classes}

In the supplementary material, we discuss the class of relational functions computable by the symbolic message-passing operation in relational abstractors, as well as the robustness of these operations. In the process, we characterize the class of relational functions realizable by inner product relational neural networks, which may be of independent interest. These results are important for appreciating the expressivity of relations and symbolic message passing, but are more technical and we therefore gather the results in the supplement. A summary of the results follows.


\def\rdot{\bigcdot}
\def\F{{\mathfrak{F}}}
\def\MLP{\text{MLP}}

\subsection{Universal approximation for inner product relations}

Our first result is a universal approximation result for inner product relations. This is useful when characterizing the class of functions computable by abstractors, but is also of interest more generally for relational machine learning.
We would like to learn a relation function \(R: \mathcal{X} \times \mathcal{X} \to \reals^{d_r}\) which maps pairs of objects in \(\mathcal{X}\) to a \(d_r\)-dimensional vector describing the relation between these objects. We model this relation function as a vector of inner products between transformations of the objects' representations,
using learnable transformations corresponding to each dimension of the relation, thought of as \textit{relational filters}. They extract a particular attribute of the objects such that an inner product of the transformed objects indicates the alignment or similarity along this attribute. Having several different filters allows for modeling rich multi-dimensional relations. This is one notable advantage of this formulation over the CoRelNet model \citep{kerg2022neural}, which processes a relation matrix as input to a multi-layer perceptron.
We make use of Mercer's theorem and universal approximation properties of feedforward networks to obtain a universal approximation result for inner product relational neural networks. This is contained in Theorem~\ref{theorem:function_class_inner_product_relnn}.

Additionally, we can consider inner products of the form
$\langle W_k^{(1)} \phi(x_i), W_k^{(2)} \phi(x_j) \rangle$
where the linear projections for the first and second entities may be different, in order to achieve non-symmetric relation functions. This yields a strictly larger class of relation functions than in Theorem \ref{theorem:function_class_inner_product_relnn}.

\subsection{Class of relational functions computable by symbolic message-passing}

Our next result characterizes the class of relational functions computable by symbolic message-passing.
Specifically, for appropriately chosen distributed symbols, 
a one-step symbolic message-passing operation can compute arbitrary functions of a each object's relations with other objects in the input sequence. That is, there exists a choice of symbols \(s_1, \ldots, s _\m\) and parameters of the feed-forward network such that abstract symbol \(a_i\) computes an arbitrary function of object \(i\)'s relations.

This observation can be applied to a simplified version of the relational  abstractor. In particular, while it captures the effects of relational cross-attention, it does not include self-attention on the abstract symbols. The analysis indicates that we should expect the function class generated by a relational  abstractor module to be no smaller than that of the simple symbolic message-passing operation.


\subsection{Composing  abstractors to compute relations on relations}
\label{ssec:compsing_abstractors}

Next, we consider, 
the function class generated by a composition of several abstractors. We make the simplifying assumption that each single layer abstractor takes the simplified form of the symbolic message-passing operation, which corresponds to omitting the self-attention operation in \Cref{alg:relational_abstractor} while maintaining the relational cross-attention with the sequence of output vectors at the previous  abstractor. In this case 
we see that chaining together a sequence of  abstractors allows the computation of relations on relations.


\subsection{Robustness and error correction}

For the relational cross-attention mechanism used by abstrators, an \(m\times m\) relation
is computed as  \(R = \mbox{Softmax}(K^T Q)\)
and relational cross attention then transforms the symbols by
\(A = SR\) so that each abstract variable \(a_j\) is in the convex hull of the set of symbols.
As long as \(S\) has rank \(m\), relations are uniquely determined from the abstract symbols.
Here we point out how the transformed symbols can be robust to noise if the symbols are
sufficiently redundant. The results of \cite{model_repair} make the robustness properties precise. We investigate robustness empirically in the experiments section.




