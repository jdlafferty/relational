\def\rdot{\bigcdot}
\def\F{{\mathfrak{F}}}
\def\MLP{\text{MLP}}

\section{Function classes}\label{sec:function_spaces}

In this section, we analyze the class of functions which can be modeled by the Abstractor module. We do this by analyzing the class of functions of its two main components: the multi-head relation module and symbolic message-passing.

We start by presenting a universal approximation result for ``inner product neural networks'', as used in the multi-head relation module. This will be useful when characterizing the class of functions computable by Abstractors, but is also of independent interest more generally for relational machine learning.

\subsection{Function class of inner product relations}

This section analyzes the class of functions which can be modeled by ``inner product relations'' and the multi-head relation module. Consider vectors living in a space \(\mathcal{X}\). We would like to learn a relation function \(r: \mathcal{X} \times \mathcal{X} \to \reals^{d_r}\) which maps pairs of objects in \(\mathcal{X}\) to a \(d_r\)-dimensional vector describing the relation between these objects. We specialize~\Cref{eq:inner_prod_rel_supp} to the symmetric form, $\phi_i = \psi_i$:
\begin{equation}
	\label{eq:symmetric_inner_product_relations}
	R(x, y) = \begin{pmatrix}\langle \phi_{1}(x), \phi_{1}(y) \rangle \\ \vdots \\ \langle \phi_{d_r}(x), \phi_{d_r}(y) \rangle \end{pmatrix},
\end{equation}
where \(\phi_{1}, \ldots, \phi_{d_r}\) are learnable transformations corresponding to each dimension of the relation.

In a deep learning model, a natural choice is for \(\phi_{1}, \ldots, \phi_{d_r}\) to be \(d_r\) different neural networks (e.g.: MLPs, CNNs, etc. depending on the object space \(\mathcal{X}\)). Hence, the parameters of \(R\) are \(\boldsymbol{\theta} = (\theta_{1}, \ldots, \theta_{d_r})\), where \(\theta_{i}\) are the parameters of \(\phi_{i}\).

The following result characterizes the class of relation functions computable by~\eqref{eq:symmetric_inner_product_relations} when \(\phi_{1}, \ldots, \phi_{d_r}\) are feedforward networks. We make use of Mercer's theorem and universal approximation properties of feedforward networks to obtain a universal approximation result for (symmetric) inner product relational neural networks.


\begin{thm}[Function class of inner product relational neural networks]
	\label{theorem:function_class_inner_product_relnn}
	\hphantom{~}

	Consider an inner product relational neural network modeling a \(d_r\)-dimensional relation via inner products of neural networks,
	\begin{equation*}
		\langle x, y \rangle_{\MLP} := \begin{bmatrix}\langle \MLP_{\theta_1}(x), \MLP_{\theta_1}(y) \rangle \\ \vdots \\ \langle \MLP_{\theta_{d_r}}(x), \MLP_{\theta_{d_r}}(y) \rangle\end{bmatrix}.
	\end{equation*}

	Suppose the data lies in a compact Hausdorff space \(\mathcal{X}\) (e.g.: a metric space) with a finite countably additive measure. In particular, \(\mathcal{X}\) can be any compact subset of \(\mathbb{R}^d\).

	Then, \(\langle \cdot, \cdot \rangle_{\MLP}\) is a Mercer kernel along each of the \(d_r\) dimensions.

	Furthermore, for any vector-valued relation function \(r: \mathcal{X} \times \mathcal{X} \to \reals^{d_r}\) which is a Mercer kernel in each dimension, there exists an inner product relational neural network which approximates \(r\) arbitrarily closely in the supremum norm (i.e.: uniformly over \((x,y) \in \mathcal{X}\times\mathcal{X}\)). More precisely, for all \(\epsilon > 0\), there exists \(d_r\) neural networks with parameters \(\theta_1, \ldots, \theta_{d_r}\) such that \(\sup_{x,y \in \mathcal{X}}{\lVert r(x,y) - \langle x, y \rangle_\MLP \rVert_\infty} < \epsilon\).
\end{thm}

\begin{proof}
	\hphantom{~}

	Denote the given relation function \(r\) by its \(d_r\) components:
	\begin{equation}
		r(x,y) = (r_1(x,y), \ldots, r_{d_r}(x,y)).
	\end{equation}
	By assumption, \(r_i\) is a Mercer kernel for each \(i = 1, \ldots, d_r\). Consider the component \(r_i\). By Mercer's theorem \citep{mercerFunctionsPositive1909, sunMercerTheorem2005, universal}, there exists \((\psi_i)_{i \in \mathbb{N}}\), \(\lambda_i \geq 0\) such that \(r_i(x,y) = \sum_{i=1}^{\infty}{\lambda_i \psi_i(x) \psi_i(y)}\), where \(\psi_i\) and \(\lambda_i\) are eigenfunctions and eigenvalues of the integral operator

	\begin{align*}
		T_r&: L_2(\mathcal{X}) \to L_2(\mathcal{X}) \\
		T_r(f) &= \int_{\mathcal{X}}{r(\cdot, x) f(x) dx}.
	\end{align*}

	Furthermore, the convergence of the series is uniform:
	\begin{equation}
		\lim_{n \to \infty} \sup_{x,y \in \mathcal{X}} \lvert r_i(x,y) - \sum_{j=1}^{n}{\lambda_j \psi_j(x) \psi_j(y) \rvert} = 0
	\end{equation}

	Let \(\tilde{n}_i\) be such that
	\begin{equation}
		\label{eq:proof_mercer_thm_unif_abs_cv}
		\sup_{x,y \in \mathcal{X}} \left\lvert r_i(x,y) - \sum_{j=1}^{n}{\lambda_j \psi_j(x) \psi_j(y)} \right\rvert < \frac{\epsilon}{2}
	\end{equation}

	Now, for \(j = 1, \ldots, \tilde{n}_i\), let the \(i\)th neural network with parameters \(\theta_i\) be a function from \(\mathcal{X}\) to \(\tilde{n}_i\)-dimensional space. Let \((\sqrt{\lambda_1} \psi_1, \ldots, \sqrt{\lambda_{\tilde{n}_i}} \psi_{\tilde{n}_i})\) be the function to be approximated by the \(i\)th neural network. By the universal approximation property of neural networks, for any \(\epsilon_1\), there exists a neural network with parameters \(\hat{\theta}_i\) such that
	\begin{equation}
		\label{eq:proof_NN_UAP}
		\sup_{x\in \mathcal{X}}{\left\lvert (\MLP(x))_j - \sqrt{\lambda_j} \psi_j(x) \right\lvert } < \epsilon_1
	\end{equation}

	We refer to \citep{hornikMultilayerFeedforward1989, cybenkoApproximationSuperpositions1989, barronUniversalApproximation1993} for results guaranteeing the existence of neural networks which can approximate any continuous function over a bounded domain.

	For ease of notation, we denote \(\MLP_{\hat{\theta}_i}\) simply by \(\MLP\), omitting the dependence on fixed \(i\). Furthermore, \(\MLP(x)_j\) is the \(j\)th component of the output of \(\MLP(x)\). Now note that the approximation error for \(r_i\) is bounded by
	\begin{equation}
		\label{eq:proof_approx_bound}
		\begin{split}
			\sup_{x, y \in \mathcal{X}}&{
				\left\lvert r_i(x,y) - \langle \MLP(x), \MLP(y) \rangle \right\rvert}\\
			&= \sup_{x, y \in \mathcal{X}}{
				\left\lvert r_i(x,y) - \sum_{j=1}^{\tilde{n}_i}{\MLP(x)_j \MLP(y)_j} \right\rvert} \\
			&\leq \sup_{x,y \in \mathcal{X}}{ \left(
				\left\lvert r_i(x,y) - \sum_{j=1}^{\tilde{n}_i}{\lambda_j \psi_j(x) \psi_j(y)} \right\rvert
				+ \left\lvert \sum_{j=1}^{\tilde{n}_i}{\lambda_i \psi_j(x) \psi_i(y) - \MLP(x)_j \MLP(y)_j} \right\rvert  \right) }
		\end{split}
	\end{equation}
	The first term is less than \(\frac{\epsilon}{2}\) by~\eqref{eq:proof_mercer_thm_unif_abs_cv}. The second term can be bounded uniformly on \(x,y\) by
	\begin{equation*}
		\begin{split}
			&\left\lvert \left(\sum_{j=1}^{\tilde{n}_i}{\lambda_i \psi_j(x) \psi_i(y)}\right) - \langle \MLP(x), \MLP(y) \rangle \right\rvert  \\
			&\leq \sum_{j=1}^{\tilde{n}_i}{ \left\lvert \lambda_i \psi_j(x) \psi_i(y) - \MLP(x)_j \MLP(y)_j \right\rvert} \\
			&\leq \sum_{j=1}^{\tilde{n}_i}{\left(
				\left\lvert \MLP(x)_j \right\rvert \left\lvert \sqrt{\lambda_j} \psi_j(y) - \MLP(y)_j \right\rvert
				+ \lvert \MLP(y)_j \rvert \left\lvert \sqrt{\lambda_j} \psi_j(x) - \MLP(x)_j \right\rvert
				\right)}
		\end{split}
	\end{equation*}
	Let \(\epsilon_1\) in \eqref{eq:proof_NN_UAP} be small enough such that the above is smaller than \(\frac{\epsilon}{2}\). 	Then, by \eqref{eq:proof_approx_bound}, we have that
	\begin{equation*}
		\sup_{x, y \in \mathcal{X}}{
			\lvert r_i(x,y) - \langle \MLP(x), \MLP(y) \rangle \rvert} \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
	\end{equation*}

	We repeat this for each component of the relation function \(r_i\), \(i=1, \ldots, d_r\), obtaining \(d_r\) neural networks each with parameters \(\hat{\theta}_i\). Thus, \(\sup_{x,y \in \mathcal{X}}{\lVert r(x,y) - \langle x, y \rangle_\MLP \rVert_\infty} < \epsilon\).
\end{proof}

\begin{remark}
	The result also holds for universal approximators other than feedforward neural networks, with a nearly identical proof.
\end{remark}

Theorem~\ref{theorem:function_class_inner_product_relnn} shows that inner products of neural networks (of the form in the multi-head relation module) can approximate arbitrary continuous, symmetric, positive semi-definite relation functions.

This theorem characterizes the class of functions that the \textit{symmetric} multi-head relation module can model. The class of functions in~\Cref{eq:inner_prod_rel_supp} is strictly larger. As mentioned in the previous section, one way to model multi-dimensional asymmetric relations is through a single `embedder' network $\phi$, and $d_r$ pairs of projection matrices $W_1^{(i)}, W_2^{(i)}, i = 1, \ldots, d_r$ (\Cref{eq:inner_prod_rel_weight_sharing}). This has the same function class as~\Cref{eq:inner_prod_rel_supp} but enables greater weight sharing. The function class of non-symmetric inner product relations can also be characterized.

\subsection{Class of relational functions computable by symbolic message-passing}\label{ssec:function_class_symbolic_mp}

In this section, we analyze the class of functions computable by symbolic message-passing as described in~\Cref{alg:symbolic_mp}. We view this operation as processing relations, mapping a relation tensor to a sequence of objects.

From equation~\eqref{eq:linear_symbolic_mp}, the symbolic message-passing operation is clearly bijective as a function on the input relation tensor \(R\), for an appropriate choice of the symbol parameters \(\bm{S} = (s_1,\ldots, s_\m)\). For example, choosing \(\bm{S} = I_{\m \times \m}\) (i.e.: the \(i\)th symbolic vector is the indicator \(\m\)-vector with a \(1\) in the \(i\)th position, \(s_i = e_i\)) reproduces the relation tensor after one message-passing operation:
\begin{equation*}
	s_i'  \leftarrow  \sum_{j=1}^{n} R[i,j] e_j = \begin{pmatrix}R[i,1] \\ R[i,2] \\ \vdots \\ R[i,n]\end{pmatrix}.
\end{equation*}
More generally, one linear step of symbolic message-passing yields updated symbolic vectors such that \(s_i'\) is a linear function of the vector containing all objects' relations with object \(i\):
\begin{equation*}
	s_i' \leftarrow \bm{S} \begin{pmatrix}R[i,1] \\ \vdots \\ R[i,n]\end{pmatrix}.
\end{equation*}
Following the linear step in symbolic message-passing, each updated symbolic state is transformed via a neural network. Hence, the \(i\)th abstracted value after symbolic message-passing is given by
\begin{equation*}
	a_i = \phi\left(\bm{S} R_i \right),
\end{equation*}
where \(\phi\) is a neural network, and \(R_i\) is the vector of object \(i\)'s relations with every other object, \(R_i = \begin{pmatrix}R[i,1] & \cdots & R[i,n]\end{pmatrix}^\top\). Hence, \(a_i\) summarizes all the information about object \(i\)'s relations to all other objects. We formalize this discussion in the following
lemma, which follows from universal approximation properties of feed-forward networks.

\begin{lemma}[Function class of symbolic message-passing]\label{lemma:function_class_1_step_symbolic_mp}
	\hphantom{~}

	A one-step symbolic message-passing operation (in \Cref{alg:symbolic_mp}) can compute arbitrary functions of a each object's relations with other objects in the input sequence. That is, there exists a choice of symbols \(s_1, \ldots, s _\m\) and parameters of the feed-forward network such that \(a_i\) computes an arbitrary function of object \(i\)'s relations, \(R_i = \begin{pmatrix}R[i,1] & R[i,2] & \cdots & R[i,n]\end{pmatrix}^\top\).
\end{lemma}


Thus, the abstracted sequence after a single step of symbolic message-passing has the form
\begin{equation}
	\label{eq:abstracted_seq_1_layer_abstractor}
	A^{(1)} = (a_1^{(1)}, \ldots, a_\m^{(1)}) = \left(\phi(R_1), \phi(R_2), \ldots, \phi(R_\m)\right),
\end{equation}
where \(\phi\) is an arbitrary learnable function shared by all abstracted objects, and \(r_i\) is the vector of object \(i\)'s relations with every other object.

That is, \(a_i^{(1)}\) summarizes object \(i\)'s relations with other objects. With further symbolic message-passing operations, the \(i\)th abstracted vector can be made to represent information about other relations, not necessarily involving the \(i\)th object. For example, at the second layer, the abstracted vectors take the form
\begin{equation}
	a_i^{(2)} = \phi^{(2)} \left( \sum_{j=1}^{n} R[i,j] a_j^{(1)} \right) = \phi^{(2)} \left( \sum_{j=1}^{n} R[i,j] \phi^{(1)}(R_j) \right).
\end{equation}

\subsection{Composing  Abstractors to compute relations on relations}\label{ssec:compsing_abstractors}

As described in \Cref{sec:abstractor_framework}, the Abstractor framework supports composing  Abstractors in the form
\begin{equation*}
	\module{Encoder} \to \module{Abstractor} \to \cdots \to \module{Abstractor} \to \module{Output}.
\end{equation*}

Here, we analyze the function class generated by a composition of several Abstractors. Thus, each Abstractor models and processes the relations between the abstract symbols at the previous Abstractor. To make the analysis tractable, we assume that each Abstractor has a single-layer.

We saw in the previous section that a one-layer Abstractor is able to compute arbitrary functions of each object's relations in the sequence. Observe that the output sequence of abstracted objects is a sequence of `relational vectors'. That is, objects which summarize relational information. Hence, chaining together a sequence of  Abstractors allows the computation of relations on relations.

% TODO: formalize or refine presentation of result
\begin{lemma}[Function class for compositions of Abstractors]\label{lemma:function_class_composed_abstractors}
	\hphantom{~}

	Consider a chain of \(k\) single-layer Abstractors each given by~\Cref{alg:abstractor},
	\[\bm{X} \to \module{Abstractor} \to \cdots \to \module{Abstractor} \to \bm{A}.\]

	A chain of \(k\) single-layer  Abstractors is able to compute arbitrary ``\(k\)th order relational functions'' in the sense of the proof below.
\end{lemma}
\begin{proof}[Proof sketch]
	In \Cref{ssec:function_class_symbolic_mp} we characterized the output of a 1-layer Abstractor as
	\begin{equation*}
		a_i^{(1)} = \phi^{(1)}\left(\bm{S}^{(1)} \begin{bmatrix}R^{(1)}[i,1] \\ \vdots \\ R^{(1)}[i,n]\end{bmatrix}\right) \equiv \phi^{(1)}\left( \bm{S}^{(1)} R_i^{(1)}\right),
	\end{equation*}

	where the relation tensor $R^{(1)}$ is computed by a multi-head relation module applied to the input object sequence $\bm{X} = (x_1, \ldots, c_\m)$.

	Note that we will now use the superscript to denote the order in the Abstractor composition chain rather than the layer depth within a single Abstractor (all  Abstractors have a depth of one).

	Let the second Abstractor's symbols be denoted by \(\bm{S}^{(2)} = (s_1^{(2)}, \ldots, s_\m^{(2)})\). Then,
	\begin{equation*}
		a_i^{(2)} = \phi^{(2)}\left(\bm{S}^{(2)} \begin{bmatrix}R^{(2)}[i,1] \\ \vdots \\ R^{(2)}[i,n]\end{bmatrix}\right),
	\end{equation*}
	where $R^{(2)}$ is the relation tensor computed by the second Abstractor's multi-head relation module:
	\begin{equation*}
		R^{(2)}[i, j, k] = \sigma_{\mathrm{rel}}^{(2)}\left(\left\langle W_1^{(k)} \phi_r^{(2)}(a_i^{(1)}), W_2^{(k)} \phi_r^{(2)}(a_j^{(1)}) \right\rangle\right), \quad i,j \in [\m], k \in \left[d_r^{(2)}\right],
	\end{equation*}

	where $i,j \in [m]$ indexes the relation pair, $k$ indexes the dimension of the relation, $W_1, W_2$ are projection matrices, $\phi_r$ is the relational filter/embedder, and $\sigma_{\mathrm{rel}}$ is the relation activation function.

	$R^{(2)}$ is a relation tensor which computes the relations between the abstract object output \textit{by the previous abstractor}, $\bm{A}^{(1)} = (a_1^{(1)}, \ldots, a_\m^{(1)})$. The space over which $R^{(2)}$ computes relations is itself a space of relation vectors. Recall that $a_i = \phi^{(1)}(\bm{S}^{(1)} R_i^{(1)})$ summarizes object $i$'s relations. Hence, $R^{(2)}[i,j]$ models the relation between object \(i\)'s relations and object \(j\)'s relations. Hence, the abstract objects at the second abstractor can be written as

	\begin{equation*}
		a_i^{(2)} = \tilde{\phi}^{(2)}\left( \begin{bmatrix}
			\left\langle \tilde{\phi}_r^{(2)}(R_i^{(1)}), \tilde{\psi}_r^{(2)}(R_1^{(1)})\right\rangle \\
			\vdots \\
			\left\langle \tilde{\phi}_r^{(2)}(R_i^{(1)}), \tilde{\psi}_r^{(2)}(R_\m^{(1)})\right\rangle
		\end{bmatrix}\right),
	\end{equation*}

	where $\tilde{\phi}_r^{(2)}, \tilde{\psi}_r^{(2)}$ are ``relational filters'' which encapsulate the projection matrices $W_1, W_2$ and the maps $\phi_r^{(1)}, \phi_r^{(2)}$, while $\tilde{\phi}^{(2)}$ encapsulates the symbols $\bm{S}^{(1)}, \bm{S}^{(2)}$ and the maps $\sigma_{\mathrm{rel}}^{(2)}, \phi^{(1)}, \phi^{(2)}$. By taking these maps to be the identity, the class of relation functions on the first-order relations $(R_1, \ldots, R_\m)$ is readily characterized by Theorem~\ref{theorem:function_class_inner_product_relnn} in terms of the parameters of the second Abstractor's multi-head relation module.

	More generally, at layer \(l\), we hav

	\begin{align*}
		R^{(l)}[i, j, k] &= \sigma_{\mathrm{rel}}^{(l)}\left(\left\langle W_1^{(k)} \phi_r^{(l)}(a_i^{(l-1)}), W_2^{(k)} \phi_r^{(l)}(a_j^{(l-1)}) \right\rangle\right), \quad i,j \in [\m], k \in \left[d_r^{(l)}\right], \\
			% R^{(l)} &= \Softmax\left((W_K A^{(l-1)})^\top (W_Q A^{(l-1)})\right),\\
		a_i^{(l)} &= \phi^{(l)}\left(\bm{S}^{(l)} R_i^{(l)}\right).
		% a_i^{(l)} &= \phi^{(l)}\left(\bm{S}^{(l)} \begin{bmatrix}R^{(l)}[i,1] \\ \vdots \\ R^{(l)}[i,n]\end{bmatrix}\right).
	\end{align*}
	Thus, \(R^{(l)}\) computes \(l\)th order-relations, and \(a_i^{(l)}\) is a function processing the \(l\)th-order relations involving object \(i\).
\end{proof}


\subsection{Robustness and error correction}

In this section, we consider the robustness of the symbolic message-passing operation to noise. Suppose a relation tensor $R$ (say computed by a multi-head relation module) is given and transformed by symbolic message-passing via~\Cref{alg:symbolic_mp}. Suppose that the output of the message-passing operation is corrupted by noise. Can the relations be recovered reliably?

In linear symbolic message-passing, the symbols are transformed by \(A = SR\) so that each abstract variable \(a_j\) is in the convex hull of the set of symbols. As long as \(S\) has rank \(m\), relations are uniquely determined from the abstract symbols. Here we point out how the transformed symbols can be robust to noise if the symbols are
sufficiently redundant.

Specifically, suppose that the symbols \(S\) are transformed to \(A\) and corrupted with additive noise:
\begin{equation}
  A = SR + \Xi
\end{equation}
where a fraction \(\epsilon\) of the entries of \(\Xi\) are drawn from an adversarial noise distribution, and the other entries are zero; dropout noise is also possible.
This can be studied as an instance of compressed sensing and ``model repair'' \citep{candes_randall,model_repair}.  In particular, the relations can be recovered using the  robust regression estimator
\begin{equation}
  \hat r_j = \argmin_{u \in\reals^m} \| a_j - S u\|_1 \label{eq:lp}
\end{equation}
where \(A = (a_1,a_2,\ldots, a_m)\) with columns \(a_j\in\reals^d\).
The main lemma in \cite{model_repair} states that the following two conditions suffice:

\underline{Condition A:}
  There exists some \(\sigma^2\), such that for any fixed \(c_1,...,c_d\) satisfying \(\max_i|c_i|\leq 1\),
  \begin{equation}
    \left\|\frac{1}{d}\sum_{i=1}^d c_i s_{i\rdot} \right\|^2\leq \frac{\sigma^2 m}{d},
  \end{equation}
with high probability, where \(s_{i\rdot}\in\reals^m\) is the \(i\)th row of \(S\).

\underline{Condition B:}
  There exist \(\underline{\kappa}\) and \(\overline{\kappa}\), such that
  \begin{eqnarray}
  \label{eq:l1-upper-A} \inf_{\|\Delta\|=1}\frac{1}{d}\sum_{i=1}^d|s_{i\rdot}^T\Delta| &\geq& \underline{\kappa}, \\
  \label{eq:l2-upper-A} \sup_{\|\Delta\|=1}\frac{1}{d}\sum_{i=1}^d|s_{i\rdot}^T\Delta|^2 &\leq& \overline{\kappa}^2,
  \end{eqnarray}
  with high probability.

\begin{thm}\label{theorem:main-improved}
  Assume the symbol matrix \(S\) satisfies Condition A and Condition B. Then if
  \begin{equation}
  \frac{\overline{\kappa}\sqrt{\frac{m}{d}\log\left(\frac{e d}{k}\right)}+\epsilon\sigma\sqrt{\frac{m}{d}}}{\underline{\kappa}(1-\epsilon)}
  \end{equation}
  is sufficiently small, the linear program \eqref{eq:lp} recovers \(R\), so that \(\hat r_j = r_j\) with high probability.
  \end{thm}

The condition is essentially that
  \begin{equation}
    \frac{1}{1-\epsilon} \sqrt{\frac{m}{d}}
  \end{equation}
  is small, meaning that the dimension \(d\) of the symbols needs to be sufficiently large relative
  to the dimension \(k\) of the relation.

  \subsection{Sparse, high-dimensional relations}

 The above setting ensures enough redundancy to recover the relations, constraining the number of symbols \(k\) to be small relative to the symbol dimension \(d\). This is not appropriate in the situation where the relations are over a large number \(m\) of elements, for example, the contents of the entire episodic memory.
 In this setting we assume that the relation tensor \(R \in \reals^{m\times m}\) is sparse; that is,
 each column \(r_j \in \Delta_m\) has at most \(k\) nonzero entries: \(\|r_j\|_0 \leq k\). To recover the relation
 we now use the robust lasso estimator, which is a related linear program
\begin{equation}
  \hat r_j = \argmin_{u \in\reals^m} \| a_j - S u\|_1 + \lambda\|u\|_1. \label{eq:rlasso}
\end{equation}
Here we have an analogous theorem, stating that if
\begin{eqnarray}
  \frac{\overline{\kappa}/\underline{\kappa}}{1-\epsilon}\sqrt{\frac{k}{d}\log(2m)}\leq c,
\end{eqnarray}
for some sufficiently small constant \(c>0\), the robust lasso estimator \eqref{eq:rlasso} satisfies
\begin{equation}
  \|\hat r_j - r_j\| \leq C \frac{\overline{\kappa}/\underline{\kappa}^2}{1-\epsilon} \sqrt{\frac{\sigma^2 k}{d} \log(2m)}
\end{equation}
for some constant \(C\).
This implies that we can accurately recover the relation tensor in the high dimensional setting, even when many of the entries of the transformed abstract symbols are corrupted.


The above discussion shows how the relation tensor can be recovered from the transformed symbols, even under adversarial noise, assuming there is sufficient redundancy in the symbols. This implies that it is possible to predict as well from the transformed symbols as from the relations, without explicitly recovering the relations.
Using ideas from \citep{surfing,HandV17}, it may be possible to extend this theory to nonlinear mappings
\(y = \varphi(Au) + \eta\) where \(\varphi(\cdot)\) is an activation function.
%; this could serve as an alternative to the Hopfield model of memory retrieval and pattern completion.

