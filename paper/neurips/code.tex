\section{Code for experiments}
\label{sec:code}

The supplementary material includes code for the experiments reported in \Cref{sec:experiments} of the paper. This code includes Python (Jupyter) notebooks for individual experiments as well as scripts that were run on a GPU cluster to parallelize across multiple experimental designs. Here, we give additional details on each experiment.

\subsection{Pairwise order relation}
The set up of the experiment is fully described in~\Cref{sec:experiments}. Here, we give details on the architecture we used.

The Abstractor model uses the implementation in \texttt{abstractor.py} which closely follows~\Cref{alg:abstractor}. The object pair are transformed by a linear embedder of dimension $64$ independently, then passed to an Abstractor module. The abstract objects output by the Abstractor are flattened to a single vector which is processed by a 32-unit dense layer with a relu activation. Finally, a 1-unit dense layer with sigmoid activation outputs the classification. The Abstractor module has 1 layer, a relation dimension of 4, a symbol dimension of 64, a projection dimension of 16, and a softmax relation activation function.

The Abstractor model is compared to CoRelNet. The same 64-dimensional linear embedder is used. This is used to compute the similarity matrix which is then flattened to form a single vector. The same 32-unit hidden layer and 1-unit classification layer are used.

We compute learning curves for each model by varying the training set size and evaluating the hold-out test set.

\subsection{Sorting random objects}

The dataset is described in~\Cref{sec:experiments}. We compare an Abstractor model, a Transformer model, and an Ablation model. The Abstractor model is of the form in~\Cref{alg:relational_abstractor}. The full autoregressive model uses the architecture $\module{Encoder} \to \module{Abstractor} \to \module{Decoder}$. For each of the Encoder, Abstractor, and Decoder modules, we use 2 layers, 2 attention heads/relation dimensions, a feedforward network with 64 units and an embedding dimension of 64. The number of trainable parameters is $386,954$. The Ablation model uses an identical architecture except that the relational attention is replaced with standard cross attention at the Encoder-Abstractor interface (with $Q \gets A, K \gets E, V \gets E$). It has the same number of parameters.

The Transformer model is standard and uses the $\module{Encoder} \to \module{Decoder}$ architecture. For both the Encoder and Decoder modules, we use 4 layers, 2 attention heads/relation dimensions, a feedforward network with 64 units and an embedding dimension of 64. The number of trainable parameters is $469,898$. We increased the number of layers compared to the Abstractor in order to make it a comparable size in terms of parameter count. Previously, we experimented with identical hyperparameters (where the Transformer would have fewer parameters due to not having an Abstractor module).

\subsection{Generalization to new object-sorting tasks}

\subsection{Robustness to noise and out-of-distribution generalization}

% \setcounter{subsection}{4}
\subsection{SET experiments}

This is a standalone implementation of an Abstractor that learns to classify triples of images of cards according 
to whether or not they form a SET, in an end-to-end fashion. First a convolutional neural network is trained to process the color images of the cards. The images are $70 \times 40$ pixels in size with $4$ color channels. The CNN has two convolutional layers, each with $32$ filters of size $5\times 5$ coupled with a $4\times 4$ max pooling layer. The feature maps are flattened and passed through two dense feedforward layers. The CNN is trained to predict the attribute of each card (one, two three; red, green, purple; empty, solid, striped; oval, diamond, squiggle), as a multi-label classification, and then an embedding of dimension $d=32$ of each card is obtained from the intermediate dense layer. 

Next, Abstractors are trained separately for each of the four attributes (number, color, pattern, shape) to learn same/different relations, where the task is to decide if an input pair of cards is the same or different for that attribute. The cards are encoded using the feature map generated from the CNN. We then use the query and key mappings $W_Q$ and $W_K$ learned for these relations to initialize the relations in a multi-head Abstractor. This Abstractor is then trained on a dataset of triples of cards, half of which form a SET, again representing the cards using the CNN feature maps.

The Abstractor is compared to a baseline model where the attributes and the relations are hand-coded symbolically, the relations are represented as vectors of 12 bits. A two-layer MLP is then trained to decide if the triple forms a SET. The MLP using the symbolic representation can be considered as a lower bound on the performance achievable by the Abstractor. We note that the simple Abstractor makes no use of self-attention or any of the other enhancements such as skip-connections commonly used in Transformers.
