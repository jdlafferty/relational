\section{Introduction}
\label{sec:intro}

* Relational learning (examples: sorting cards, other examples, RPM)

* Range of recent work on relational learning (Santori, ESBN, CorelNet)

Recent work (e.g.,~\cite{TEM, NTM,episodicControl,esbn,mondal23learned,musslick2021rationalizing}) 
has explored different forms of this inductive bias. In this paper we some of these elements into a single 
transformer-based architecture. The proposed abstractor framework extends ESBN to enable more complex, higher order forms of relations by combining it with both the attentional mechanisms and hierarchical structure of transformers. This creates a powerful combination of deep learning and relational learning that enables abstraction and generalization from limited data.


* Here we propose a framework that casts relational learning in terms of transformers. 

The success of transformers lies in combining the function approximation capabilities of deep learning with the use of attentional mechanisms to support richly context-sensitive processing \citep{transformers,vaswani2017attention,kerg2020untangling}. Yet it is clear that transformers are missing core capabilities required for modeling human thought (e.g., \cite{mahowald2023dissociating}).  In particular, they lack mechanisms required to emulate forms of flexibility and efficiency exhibited by the human brain, including an ability to support analogy and abstraction from limited data. The algorithmic challenge is to provide ways of binding domain-specific information to low dimensional, abstract representations that can be used to compute a given function in any setting for which it is relevant. 

Our approach is motivated by a type of inductive bias for learning architectures we call the ``relational bottleneck," which is motivated by principles of brain organization. Specifically, research in cognitive neuroscience has 
shed light on the subsystems that are involved when natural intelligence shows an ability to flexibly generalize abstract structure across domains of processing, while also being able to learn complex representations and functions in specific domains and compute these with remarkable efficiency.  The framework of 
complementary learning systems \citep{McClelland:1995, Kumaran:2016} describes two distinct types of neural mechanisms for learning and memory around which the brain is organized, implementing a tradeoff between slow, incremental forms of learning required to encode stable statistical structure present in the environment (semantic memory), and the ability to rapidly encode and remember novel associations (episodic memory). In its most basic and simplified form, the ``relational bottleneck'' imposes an inductive bias that limits the flow of information from sensory subsystems to reasoning and decision making subsystems to relations, as computed through inner products between distributed representations. In this paper we present a framework that casts this inductive bias in terms of 
an extension of transformers, where specific types of attention mechanisms enforce the relational bottleneck, 
and certain types of layers implement a form of abstraction and relational reasoning.

Our approach was developed while thinking about the ESBN framework \citep{esbn}, which can be seen as 
closely related to the neural Turing machine \citep{NTM}. Both frameworks augment traditional models such as 
LSTMs with external stores that can be viewed as computational models of episodic memory. However, the ESBN framework enforces a relational bottleneck by dividing the external memory into ``sensory'' and ``abstract'' sides, with lookups on the sensory side carried out using inner products. The key observation we make is that by viewing episodic  
memory query-key lookups in terms of cross attention operations, the relational bottleneck can be 
naturally implemented in an extension of transformers. Our proposed \textit{abstractor} framework extends ESBN to enable more complex, higher order forms of relations by combining it with both the attentional mechanisms and hierarchical structure of transformers. This creates a powerful combination of deep learning and relational learning that enables abstraction and generalization from limited data.




