\section{Introduction: The Relational Bottleneck}

\section{Relational Abstracters}

\noindent\textbf{Symbols} ...

\noindent\textbf{Relational Cross-Attention} ...

\noindent\textbf{Relational-Symbolic Message-Passing} ...

\section{Relational Abstracters as modules in a Transformer}
\subsection{Relational Classification Tasks}
$$\texttt{Encoder} \to \texttt{Abstracter}$$

\subsection{Sequence-to-Sequence Relational Tasks}
$$\texttt{Encoder} \to \texttt{Abstracter} \to \texttt{Decoder}$$

\noindent\textbf{Fully Relational Tasks}: decoder can attend only to abstracter. only needs relational information.

\noindent\textbf{Partially-Relational Tasks}: decoder attends to both abstracter and encoder. relational information is important to solving task, but `value' information and more general sequence-modeling is also relevant. Hence, attend to both encoder (as in standard transformer) and relational abstracter (as in above models). 

\subsection{Multi-Attention Decoders: Attending to both Encoder and Abstracter}

\subsection{Chaining Abstracters to Learn Higher-Order Relations}

\section{Characterizing the Function Class Generated by Relational-Symbolic Message-Passing}
\label{sec:function_spaces}

\section{Experiments: Sorting Random-Objects}
\label{sec:experiments}

\begin{itemize}
    \item sorting is a natural `relational' task: relations = ordering.
    \item consider a set of $N$ objects, $\{o_1, ..., o_N\}$ with associated ordering $o_1 \prec o_2 \prec \cdots \prec o_N$. objects are described by, for example, random vectors (or images, etc.). task: given a subset of the objects, sort it. requires understanding the relations between these objects.
    \item describe experimental set up and present results 
\end{itemize}

\section{Discussion}

\section*{Code and Data Availability}


