\documentclass[12pt,pdftex,noinfoline]{imsart}

\RequirePackage[OT1]{fontenc}
\usepackage{amsthm,amsbsy,amsmath,amsfonts,natbib,mathtools,amssymb}
\usepackage{mathrsfs}
\RequirePackage{hypernat}
\usepackage{graphicx}
\usepackage{times}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\hypersetup{citecolor=MidnightBlue}
\hypersetup{linkcolor=MidnightBlue}
\hypersetup{urlcolor=MidnightBlue}
\usepackage{fullpage}
\usepackage[margin=1in,footskip=.40in]{geometry}
\usepackage{amsmath, amssymb}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\renewcommand{\d}{\mathrm{d}}
\newcommand{\E}{\operatorname{\mathbb E}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\def\given{\,|\,}
\def\P{\mathbb{P}}
\def\E{\mathbb{E}}
\def\reals{\mathbb{R}}
\let\tilde\widetilde
\def\softmax#1{\mbox{Softmax}\left(#1\right)}
\def\task#1{\vskip5pt\noindent{\it\bfseries #1.}}
\def\qkv#1#2#3{\mbox{CrossAttention}(Q\!\leftarrow\!#1,\; K\!\leftarrow\!#2,\; V\!\leftarrow\!#3)}
\def\selfattention#1{\mbox{SelfAttention}(#1)}
\def\crossattention#1#2#3{\qkv{#1}{#2}{#3}}
\def\crossattend#1#2#3#4{\mbox{CrossAttention}_{#4}(Q\!\leftarrow\!#1,\; K\!\leftarrow\!#2,\; V\!\leftarrow\!#3)}

\renewcommand{\baselinestretch}{1.05}
\parskip12pt
\parindent0pt

\newcommand{\MLP}{\text{\sc mlp}}
\newcommand{\FeedForward}{\text{FeedForward}}
\newcommand{\Softmax}{\text{Softmax}}
\def\S{{\mathcal{S}}}
\def\next#1{{\text{\rm next}(#1)}}


\begin{document}

\section*{Revised argsort experiments}

\underline{Task A:} Learn the ordering for a given set of objects.

Fix the number of objects $N$, for example $N=64$.

Given a set of objects $o_1, \ldots, o_{N}$ with $o_j\in\reals^{d_o}$ (e.g. ``a particular deck of cards''), 
the task is to learn embeddings $E_1,\ldots, E_{N}$ with $E_j\in\reals^d$ that predict the next object in a total ordering. The ordering is given by a permutation $\pi \in \S_{N}$ on $N$ objects: $\pi(j)$ is the rank of object $j$.

Training data are pairs $(o_j, o_\next{j})$ where the index of the successor is 
\begin{align}
    \next{j} &= \begin{cases}
        \pi^{-1}(\pi(j)+1) & \text{if $\pi(j)<N$}\\
        \pi^{-1}(1) & \text{otherwise}.
    \end{cases}
\end{align}
The objective is to learn embeddings 
$E(o_j) = E_j\in\reals^d$ that can predict the next object. This could be formulated as an autoencoder; a simpler 
version is to predict the index of the successor.

A1--Non-relational model: Train embeddings with the model 
\begin{equation}
    \P(\next{j}=k \given j) = \softmax{f(E_j, E_{\bigcdot})}_k = \frac{\exp(f(E_j, E_k))}{\sum_{k'=1}^N \exp(f(E_j, E_{k'}))}
\end{equation}
where $f$ is a pairwise neural network.

A2--Relational bottleneck: Train embeddings with the model
\begin{equation}
 \P(\next{j}=k \given j) = \softmax{E_j^T E_{\bigcdot}}_k = \frac{\exp(E_j^T E_k)}{\sum_{k'=1}^N \exp(E_j^T E_{k'})}.
\end{equation}


Intuition: The relational bottleneck will pressure the A2 embeddings to take a canonical form that  
encodes relations approximately the same way across different problem instances.


\vskip10pt
\underline{Task B:} Learn to (arg)sort sequences of $m$ objects.

Fix $m$, for example $m=7$. 

Now we use the embeddings learned in Task A to sort. This is a seq2seq problem where an input instance is a sequence of objects $(x_1,x_2,\ldots, x_m)$ and the target is a permutation $\sigma \in \S_m$, such that the sorted order 
is $(x_{\sigma(1)}, x_{\sigma(2)}, \ldots, x_{\sigma(m)})$.


B1--Transformer. Represent the objects using the embeddings learned in Task A1. Use a standard transformer 
archictecture with self-attention in the encoder and cross-attention from encoder to decoder. 

B2--Abstractor. Represent the objects using the embeddings learned in Task A2. Use an abstractor 
with a trivial encoder that only uses the embeddings directly, without self-attention or multiple layers. 
This will keep the number of parameters comparable to the transformer in B1.

B2'--Pre-trained Abstractor. Train the Abstractor on one set of objects/embeddings, then fix it (or initialize with it) and apply to a new set of objects/embeddings.


\end{document}